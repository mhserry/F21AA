{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "import seaborn as sb\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from numba import vectorize\n",
    "import unidecode\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "import spacy\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "\n",
    "\n",
    "import string,lxml,bs4,nltk\n",
    "from warnings import simplefilter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0- Functions & Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {\n",
    "\"ain't\": \"are not\",\n",
    "\"aren't\": \"am not\",\n",
    "\"can't\": \"can not\",\n",
    "\"can't've\": \"can not have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"I had\",\n",
    "\"i'd've\": \"I would have\",\n",
    "\"i'll\": \"I will\",\n",
    "\"i'll've\": \"I will have\",\n",
    "\"i'm\": \"I am\",\n",
    "\"i've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so is\",\n",
    "\"that'd\": \"that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they had\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: to load file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will load file and return it as Data Frame and also return list of column names\n",
    "def loadFile(fileName):\n",
    "    #reading the temp file to a dataframe with the new headers    \n",
    "    print('Loading File...',end='')\n",
    "    result = pd.read_csv(fileName)\n",
    "    print('[ok]')\n",
    "    print('Loaded {:,} reviews' .format(len(result)))    \n",
    "    print('Column Names: ', result.columns.tolist())\n",
    "    return result, result.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: to drop unused columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This fuction will return modified dataframe with selected columns only\n",
    "def neededColumnsOnly(df, columns_needed):\n",
    "    #return df.filter(columns_needed) \n",
    "    #Or\n",
    "    to_drop_columns = list(x for x in df.columns.tolist() if x not in columns_needed)\n",
    "    df.drop(to_drop_columns,axis=1,inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: to find columns containing nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This fuction will check which columns having null values\n",
    "def columns_with_nulls(df, index_column):\n",
    "    null_columns = []\n",
    "    for column in df.columns:\n",
    "        if column != index_column:\n",
    "            check_null = df.isnull()[[column,index_column]].groupby(column).agg('count')\n",
    "            try:\n",
    "                if check_null.loc[True][0] > 0:\n",
    "                    null_columns.append(column)\n",
    "            except:\n",
    "                pass\n",
    "    return null_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: to remove all null values from columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This fuction will remove all nulls in columns found earlier\n",
    "def remove_nulls(df, columns):\n",
    "    for column in columns:\n",
    "        df[column].fillna('', inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: to find columns containing html tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This fuction will check which columns having html tags\n",
    "def columns_with_html(df):\n",
    "    null_columns = []\n",
    "    for column in df.columns:\n",
    "        try:\n",
    "            text_html = df[column].str.find('<')\n",
    "            text_html = text_html[text_html != -1]\n",
    "            check_null = len(text_html)\n",
    "            if check_null > 0:\n",
    "                null_columns.append(column)\n",
    "        except:\n",
    "            pass\n",
    "    return null_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: to remove html tags from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This fuction will remove all html tags in text\n",
    "def remove_html_tags(text):\n",
    "    sp = BeautifulSoup(text, \"html.parser\")\n",
    "    returned_text = sp.get_text(separator=\" \")\n",
    "    return returned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: to merge Summary and Text in one column and get rid of productId column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This fuction will result in two columns only score and review\n",
    "def merge_summary_text(df):\n",
    "    df['review']=df['Summary']+' '+df['Text'] \n",
    "    df.drop(['Summary','Text','ProductId'],axis=1,inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: to correct accent in letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will replace accents in letters with regular letters example nescafé will be nescafe\n",
    "def correct_accent(text):\n",
    "    returned_text = unidecode.unidecode(text)\n",
    "    return returned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: to visualized score count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_score_count(df):\n",
    "    # visualize total review by score count\n",
    "    final_df_grouped = df[['Score', 'review']].groupby('Score').agg('count')\n",
    "    final_df_grouped= final_df_grouped.reset_index()\n",
    "    f, ax = plt.subplots(figsize=(8, 6))\n",
    "    fig = sb.barplot(x='Score', y=\"review\", data=final_df_grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: to remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will remove punctiatons\n",
    "def remove_punctuations(text):\n",
    "    for punc in string.punctuation.replace(\"'\",\"\"):\n",
    "        if punc in text:\n",
    "            text = text.replace(punc,\" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: to remove extra spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will remove extra spaces\n",
    "def remove_extra_space(text):\n",
    "    removed_space = \" \".join(text.split())\n",
    "    return removed_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: to expand short words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will expand short words such as isn't => is not\n",
    "def expand_words(text):\n",
    "    for word in text.split():\n",
    "        if word.lower() in contractions:\n",
    "            text = text.replace(word, contractions[word.lower()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: to replace remaining punctuation '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will remove '\n",
    "def remove_apostrophe(text):\n",
    "    text = text.replace(\"'\", \" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: to remove english stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will remove stop words except no and not since they will effect our classification\n",
    "def remove_stop_words(text):\n",
    "    sWords = set(stopwords.words('english'))\n",
    "    sWords = list(sWords)\n",
    "    deselect_stop_words = ['no', 'not']\n",
    "\n",
    "    for item in deselect_stop_words:\n",
    "        sWords.remove(item)\n",
    "    for word in sWords:\n",
    "        text = text.replace(\" \" + word.lower() + \" \",\" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: to transform all to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toLower(text):\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: to tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokeniz(text):\n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: to stemm text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemm(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed = [stemmer.stem(word) for word in text.split()]\n",
    "    stemmed = ' '.join(stemmed) \n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: to lemmetize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemm(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmed = [lemmatizer.lemmatize(word) for word in text.split()]\n",
    "    lemmed = ' '.join(lemmed) \n",
    "    return lemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: to lemm text using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_lemm(text):\n",
    "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "    doc = nlp(text)\n",
    "    lemmed = [token.lemma_ for token in doc]\n",
    "    lemmed = ' '.join(lemmed) \n",
    "    return lemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: to Represent Vector Count Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this fuction will take the type of model (CountVector or TFIDF) and will return train data, feature name and vector repr. matrix\n",
    "\n",
    "def VCM (vectorizer,df, colum_name, class_column, ngram_min = 1, ngram_max = 1):\n",
    "    if vectorizer == \"count\":\n",
    "        vect = CountVectorizer(ngram_range=(ngram_min,ngram_max)).fit(df[colum_name])\n",
    "    elif vectorizer == \"tfidf\":\n",
    "        vect = TfidfVectorizer(ngram_range=(ngram_min,ngram_max)).fit(df[colum_name])\n",
    "    \n",
    "    x_Train = vect.transform(df[colum_name])\n",
    "    y_Train = df[class_column]\n",
    "    feature_names = vect.get_feature_names()\n",
    "    dense_vect = x_Train.todense()\n",
    "    dense_list = dense_vect.tolist()\n",
    "    vectDF = pd.DataFrame(dense_list, columns=feature_names)\n",
    "    return x_Train, y_Train, vect, feature_names, vectDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Data Exploration and Visualization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Data Preparation and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Load Data File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading File...[ok]\n",
      "Loaded 426,340 reviews\n",
      "Column Names:  ['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text']\n"
     ]
    }
   ],
   "source": [
    "# Load Data Set, this will return our main Data set and list of column names\n",
    "\n",
    "original_df, column_names = loadFile('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Extract Needed Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of un needed columns\n",
    "\n",
    "df_required_columns = neededColumnsOnly(original_df, ['ProductId','Score','Summary','Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductId</th>\n",
       "      <th>Score</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B0034EDLS2</td>\n",
       "      <td>5</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>I received this product early from the seller!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B001I7HJE4</td>\n",
       "      <td>5</td>\n",
       "      <td>Organic, Kosher, Tasty Assortment of Premium T...</td>\n",
       "      <td>*****&lt;br /&gt;Numi's Collection Assortment Melang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B000LKTB90</td>\n",
       "      <td>5</td>\n",
       "      <td>excellent gluten-free spaghetti: great taste, ...</td>\n",
       "      <td>I was very careful not to overcook this pasta,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B001HXJPS2</td>\n",
       "      <td>5</td>\n",
       "      <td>Lindt is Lindt</td>\n",
       "      <td>Buying this multi-pack I was misled by the pic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B006H34CUS</td>\n",
       "      <td>5</td>\n",
       "      <td>YUM!!!!!</td>\n",
       "      <td>These bars are so good! I loved them warmed up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B004728MI4</td>\n",
       "      <td>5</td>\n",
       "      <td>Delicious</td>\n",
       "      <td>I love these chips, I buy the 24 pack once a m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B001BZ5EFE</td>\n",
       "      <td>5</td>\n",
       "      <td>Tastes great and it's organic!!</td>\n",
       "      <td>I'm a huge fan of eating cereal for breakfast....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B0001AVRQK</td>\n",
       "      <td>2</td>\n",
       "      <td>Poor taste</td>\n",
       "      <td>I was really disappointed with the Sorghum we ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>B005GV9RZC</td>\n",
       "      <td>3</td>\n",
       "      <td>Better than US Instant Coffee</td>\n",
       "      <td>A friend who has gone to Korea gave me a coupl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>B004FEN3GK</td>\n",
       "      <td>4</td>\n",
       "      <td>Hard not to like!</td>\n",
       "      <td>No need for plastic baggies or sloppy tin foil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ProductId  Score                                            Summary  \\\n",
       "0  B0034EDLS2      5                                          Very Good   \n",
       "1  B001I7HJE4      5  Organic, Kosher, Tasty Assortment of Premium T...   \n",
       "2  B000LKTB90      5  excellent gluten-free spaghetti: great taste, ...   \n",
       "3  B001HXJPS2      5                                     Lindt is Lindt   \n",
       "4  B006H34CUS      5                                           YUM!!!!!   \n",
       "5  B004728MI4      5                                          Delicious   \n",
       "6  B001BZ5EFE      5                    Tastes great and it's organic!!   \n",
       "7  B0001AVRQK      2                                         Poor taste   \n",
       "8  B005GV9RZC      3                      Better than US Instant Coffee   \n",
       "9  B004FEN3GK      4                                  Hard not to like!   \n",
       "\n",
       "                                                Text  \n",
       "0  I received this product early from the seller!...  \n",
       "1  *****<br />Numi's Collection Assortment Melang...  \n",
       "2  I was very careful not to overcook this pasta,...  \n",
       "3  Buying this multi-pack I was misled by the pic...  \n",
       "4  These bars are so good! I loved them warmed up...  \n",
       "5  I love these chips, I buy the 24 pack once a m...  \n",
       "6  I'm a huge fan of eating cereal for breakfast....  \n",
       "7  I was really disappointed with the Sorghum we ...  \n",
       "8  A friend who has gone to Korea gave me a coupl...  \n",
       "9  No need for plastic baggies or sloppy tin foil...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_required_columns.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 Remove Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of columns containing null values\n",
    "\n",
    "list_columns_with_nulls = columns_with_nulls(df_required_columns, 'ProductId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Summary']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_columns_with_nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace null values with empty string\n",
    "\n",
    "df_without_nulls = remove_nulls(df_required_columns, list_columns_with_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductId</th>\n",
       "      <th>Score</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B0034EDLS2</td>\n",
       "      <td>5</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>I received this product early from the seller!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B001I7HJE4</td>\n",
       "      <td>5</td>\n",
       "      <td>Organic, Kosher, Tasty Assortment of Premium T...</td>\n",
       "      <td>*****&lt;br /&gt;Numi's Collection Assortment Melang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B000LKTB90</td>\n",
       "      <td>5</td>\n",
       "      <td>excellent gluten-free spaghetti: great taste, ...</td>\n",
       "      <td>I was very careful not to overcook this pasta,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B001HXJPS2</td>\n",
       "      <td>5</td>\n",
       "      <td>Lindt is Lindt</td>\n",
       "      <td>Buying this multi-pack I was misled by the pic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B006H34CUS</td>\n",
       "      <td>5</td>\n",
       "      <td>YUM!!!!!</td>\n",
       "      <td>These bars are so good! I loved them warmed up...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ProductId  Score                                            Summary  \\\n",
       "0  B0034EDLS2      5                                          Very Good   \n",
       "1  B001I7HJE4      5  Organic, Kosher, Tasty Assortment of Premium T...   \n",
       "2  B000LKTB90      5  excellent gluten-free spaghetti: great taste, ...   \n",
       "3  B001HXJPS2      5                                     Lindt is Lindt   \n",
       "4  B006H34CUS      5                                           YUM!!!!!   \n",
       "\n",
       "                                                Text  \n",
       "0  I received this product early from the seller!...  \n",
       "1  *****<br />Numi's Collection Assortment Melang...  \n",
       "2  I was very careful not to overcook this pasta,...  \n",
       "3  Buying this multi-pack I was misled by the pic...  \n",
       "4  These bars are so good! I loved them warmed up...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_without_nulls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double check if removed\n",
    "\n",
    "columns_with_nulls(df_without_nulls, 'ProductId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 Remove Html Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of columns containing html tags\n",
    "\n",
    "list_columns_with_html = columns_with_html(df_without_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Text']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_columns_with_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'*****<br />Numi\\'s Collection Assortment Melange includes:<br />5 Herbal Teas (caffeine-free, also called \"teasans\"):<br />* Dry Desert Lime: Lime Herbal Teasan<br />* Fields of Gold: Lemongrass Herbal Teasan<br />* Red Mellow Bush: Rooibos Herbal Teasan<br />* Bushmen\\'s Brew: Honeybush Herbal Teasan<br />* Simply Mint: Moroccan Mint<br /><br />2 Green Teas (lower in caffeine):<br />* Temple of Heaven: Gunpowder Green Tea<br />* Monkey King: Jasmine Green Tea<br /><br />2 Black Teas (contain caffeine):<br />* Chinese Breakfast: Yunnan Black Tea<br />* Smoky Tarry: Lapsang Souchong Black Tea<br /><br />This is a total of nine different teas, two tea bags of each one in each box. Numi teas are known for their high-quality, organic and kosher ingredients, and in my opinion, are some of the tastiest and best teas I have ever tried. They do not include artificial ingredients or flavorings.<br /><br />On the box, the manufacturer writes: \"From mist-covered mountains to sun-drenched deserts to fertile fields, we proudly bring you our tea palette. The flavors range from smooth earthiness, and light floral scents, to refreshingly sweet and sour notes. What they all share is the recollection of how some Ancient One tamed fire and water to coexist harmoniously, and steeped in them the gifts of Nature. Since that distant past, people have soothed, roused, healed, explored and celebrated with these wonderful infusions. We encourage you to do the same with Numi\\'s finest.\"<br /><br />The price is perfect, a big savings over single boxes. If you like Numi teas and want to sample a wonderful assortment, get it---you won\\'t be sorry. There may be some you like and others you don\\'t, but sampling them will be a delightful experience. If you are new to Numi this is an excellent way to sample their high-quality teas. I do not think you could find a better source of premium tea than Numi.<br /><br />Highly recommended.<br />*****'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of html tag in text column\n",
    "\n",
    "df_without_nulls.loc[1]['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_html = df_without_nulls.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of html tags \n",
    "df_without_html['Text'] = df_without_html['Text'].apply(remove_html_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'***** Numi\\'s Collection Assortment Melange includes: 5 Herbal Teas (caffeine-free, also called \"teasans\"): * Dry Desert Lime: Lime Herbal Teasan * Fields of Gold: Lemongrass Herbal Teasan * Red Mellow Bush: Rooibos Herbal Teasan * Bushmen\\'s Brew: Honeybush Herbal Teasan * Simply Mint: Moroccan Mint 2 Green Teas (lower in caffeine): * Temple of Heaven: Gunpowder Green Tea * Monkey King: Jasmine Green Tea 2 Black Teas (contain caffeine): * Chinese Breakfast: Yunnan Black Tea * Smoky Tarry: Lapsang Souchong Black Tea This is a total of nine different teas, two tea bags of each one in each box. Numi teas are known for their high-quality, organic and kosher ingredients, and in my opinion, are some of the tastiest and best teas I have ever tried. They do not include artificial ingredients or flavorings. On the box, the manufacturer writes: \"From mist-covered mountains to sun-drenched deserts to fertile fields, we proudly bring you our tea palette. The flavors range from smooth earthiness, and light floral scents, to refreshingly sweet and sour notes. What they all share is the recollection of how some Ancient One tamed fire and water to coexist harmoniously, and steeped in them the gifts of Nature. Since that distant past, people have soothed, roused, healed, explored and celebrated with these wonderful infusions. We encourage you to do the same with Numi\\'s finest.\" The price is perfect, a big savings over single boxes. If you like Numi teas and want to sample a wonderful assortment, get it---you won\\'t be sorry. There may be some you like and others you don\\'t, but sampling them will be a delightful experience. If you are new to Numi this is an excellent way to sample their high-quality teas. I do not think you could find a better source of premium tea than Numi. Highly recommended. *****'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_without_html.loc[1]['Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.4 Merge Summary and Text in review column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final dataframe preparation\n",
    "\n",
    "final_df = merge_summary_text(df_without_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Very Good I received this product early from t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Organic, Kosher, Tasty Assortment of Premium T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>excellent gluten-free spaghetti: great taste, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Lindt is Lindt Buying this multi-pack I was mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>YUM!!!!! These bars are so good! I loved them ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Delicious I love these chips, I buy the 24 pac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>Tastes great and it's organic!! I'm a huge fan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>Poor taste I was really disappointed with the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>Better than US Instant Coffee A friend who has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>Hard not to like! No need for plastic baggies ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score                                             review\n",
       "0      5  Very Good I received this product early from t...\n",
       "1      5  Organic, Kosher, Tasty Assortment of Premium T...\n",
       "2      5  excellent gluten-free spaghetti: great taste, ...\n",
       "3      5  Lindt is Lindt Buying this multi-pack I was mi...\n",
       "4      5  YUM!!!!! These bars are so good! I loved them ...\n",
       "5      5  Delicious I love these chips, I buy the 24 pac...\n",
       "6      5  Tastes great and it's organic!! I'm a huge fan...\n",
       "7      2  Poor taste I was really disappointed with the ...\n",
       "8      3  Better than US Instant Coffee A friend who has...\n",
       "9      4  Hard not to like! No need for plastic baggies ..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.5 Correct accented letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if accent exist\n",
    "text_html = final_df['review'].str.find('é')\n",
    "text_html = text_html[text_html != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "864       173\n",
       "3467      371\n",
       "4554     1236\n",
       "4590     1494\n",
       "7852       15\n",
       "9339      123\n",
       "10269     107\n",
       "12255     112\n",
       "14951    1547\n",
       "15712      61\n",
       "Name: review, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_html.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"not my cup of coffee i bought this becuase i liked the nescafé packets of instant coffee that i got in my hotel room the last time i was in the dominican republic and i'll admit, i don't like coffee. but when i tried this stuff from dominican i was like omg this is a party in my mouth and i'm drinking it black!!! so i bought this nescafé canister hoping it would be the same. well it wasn't. this stuff has nothing to recommend it. it's worse than most coffees i've tried.\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['review'].iloc[15712] #nescafé  is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove accents\n",
    "final_df['review'] = final_df['review'].apply(correct_accent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"not my cup of coffee i bought this becuase i liked the nescafe packets of instant coffee that i got in my hotel room the last time i was in the dominican republic and i'll admit, i don't like coffee. but when i tried this stuff from dominican i was like omg this is a party in my mouth and i'm drinking it black!!! so i bought this nescafe canister hoping it would be the same. well it wasn't. this stuff has nothing to recommend it. it's worse than most coffees i've tried.\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#double check if removed\n",
    "final_df['review'].iloc[15712]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.6 Remove Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuations\n",
    "final_df['review'] = final_df['review'].apply(remove_punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Organic  Kosher  Tasty Assortment of Premium Teas   Teasans       Numi's Collection Assortment Melange includes  5 Herbal Teas  caffeine free  also called  teasans      Dry Desert Lime  Lime Herbal Teasan   Fields of Gold  Lemongrass Herbal Teasan   Red Mellow Bush  Rooibos Herbal Teasan   Bushmen's Brew  Honeybush Herbal Teasan   Simply Mint  Moroccan Mint 2 Green Teas  lower in caffeine     Temple of Heaven  Gunpowder Green Tea   Monkey King  Jasmine Green Tea 2 Black Teas  contain caffeine     Chinese Breakfast  Yunnan Black Tea   Smoky Tarry  Lapsang Souchong Black Tea This is a total of nine different teas  two tea bags of each one in each box  Numi teas are known for their high quality  organic and kosher ingredients  and in my opinion  are some of the tastiest and best teas I have ever tried  They do not include artificial ingredients or flavorings  On the box  the manufacturer writes   From mist covered mountains to sun drenched deserts to fertile fields  we proudly bring you our tea palette  The flavors range from smooth earthiness  and light floral scents  to refreshingly sweet and sour notes  What they all share is the recollection of how some Ancient One tamed fire and water to coexist harmoniously  and steeped in them the gifts of Nature  Since that distant past  people have soothed  roused  healed  explored and celebrated with these wonderful infusions  We encourage you to do the same with Numi's finest   The price is perfect  a big savings over single boxes  If you like Numi teas and want to sample a wonderful assortment  get it   you won't be sorry  There may be some you like and others you don't  but sampling them will be a delightful experience  If you are new to Numi this is an excellent way to sample their high quality teas  I do not think you could find a better source of premium tea than Numi  Highly recommended       \""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.iloc[1][\"review\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.7 Expand Short Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['review'] = final_df['review'].apply(expand_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Organic  Kosher  Tasty Assortment of Premium Teas   Teasans       Numi's Collection Assortment Melange includes  5 Herbal Teas  caffeine free  also called  teasans      Dry Desert Lime  Lime Herbal Teasan   Fields of Gold  Lemongrass Herbal Teasan   Red Mellow Bush  Rooibos Herbal Teasan   Bushmen's Brew  Honeybush Herbal Teasan   Simply Mint  Moroccan Mint 2 Green Teas  lower in caffeine     Temple of Heaven  Gunpowder Green Tea   Monkey King  Jasmine Green Tea 2 Black Teas  contain caffeine     Chinese Breakfast  Yunnan Black Tea   Smoky Tarry  Lapsang Souchong Black Tea This is a total of nine different teas  two tea bags of each one in each box  Numi teas are known for their high quality  organic and kosher ingredients  and in my opinion  are some of the tastiest and best teas I have ever tried  They do not include artificial ingredients or flavorings  On the box  the manufacturer writes   From mist covered mountains to sun drenched deserts to fertile fields  we proudly bring you our tea palette  The flavors range from smooth earthiness  and light floral scents  to refreshingly sweet and sour notes  What they all share is the recollection of how some Ancient One tamed fire and water to coexist harmoniously  and steeped in them the gifts of Nature  Since that distant past  people have soothed  roused  healed  explored and celebrated with these wonderful infusions  We encourage you to do the same with Numi's finest   The price is perfect  a big savings over single boxes  If you like Numi teas and want to sample a wonderful assortment  get it   you will not be sorry  There may be some you like and others you do not  but sampling them will be a delightful experience  If you are new to Numi this is an excellent way to sample their high quality teas  I do not think you could find a better source of premium tea than Numi  Highly recommended       \""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.iloc[1][\"review\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.8 Remove remaining apostrophe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['review'] = final_df['review'].apply(remove_apostrophe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Organic  Kosher  Tasty Assortment of Premium Teas   Teasans       Numi s Collection Assortment Melange includes  5 Herbal Teas  caffeine free  also called  teasans      Dry Desert Lime  Lime Herbal Teasan   Fields of Gold  Lemongrass Herbal Teasan   Red Mellow Bush  Rooibos Herbal Teasan   Bushmen s Brew  Honeybush Herbal Teasan   Simply Mint  Moroccan Mint 2 Green Teas  lower in caffeine     Temple of Heaven  Gunpowder Green Tea   Monkey King  Jasmine Green Tea 2 Black Teas  contain caffeine     Chinese Breakfast  Yunnan Black Tea   Smoky Tarry  Lapsang Souchong Black Tea This is a total of nine different teas  two tea bags of each one in each box  Numi teas are known for their high quality  organic and kosher ingredients  and in my opinion  are some of the tastiest and best teas I have ever tried  They do not include artificial ingredients or flavorings  On the box  the manufacturer writes   From mist covered mountains to sun drenched deserts to fertile fields  we proudly bring you our tea palette  The flavors range from smooth earthiness  and light floral scents  to refreshingly sweet and sour notes  What they all share is the recollection of how some Ancient One tamed fire and water to coexist harmoniously  and steeped in them the gifts of Nature  Since that distant past  people have soothed  roused  healed  explored and celebrated with these wonderful infusions  We encourage you to do the same with Numi s finest   The price is perfect  a big savings over single boxes  If you like Numi teas and want to sample a wonderful assortment  get it   you won t be sorry  There may be some you like and others you don t  but sampling them will be a delightful experience  If you are new to Numi this is an excellent way to sample their high quality teas  I do not think you could find a better source of premium tea than Numi  Highly recommended       '"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.iloc[1][\"review\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.9 Remove Extra Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['review'] = final_df['review'].apply(remove_extra_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Organic Kosher Tasty Assortment of Premium Teas Teasans Numi s Collection Assortment Melange includes 5 Herbal Teas caffeine free also called teasans Dry Desert Lime Lime Herbal Teasan Fields of Gold Lemongrass Herbal Teasan Red Mellow Bush Rooibos Herbal Teasan Bushmen s Brew Honeybush Herbal Teasan Simply Mint Moroccan Mint 2 Green Teas lower in caffeine Temple of Heaven Gunpowder Green Tea Monkey King Jasmine Green Tea 2 Black Teas contain caffeine Chinese Breakfast Yunnan Black Tea Smoky Tarry Lapsang Souchong Black Tea This is a total of nine different teas two tea bags of each one in each box Numi teas are known for their high quality organic and kosher ingredients and in my opinion are some of the tastiest and best teas I have ever tried They do not include artificial ingredients or flavorings On the box the manufacturer writes From mist covered mountains to sun drenched deserts to fertile fields we proudly bring you our tea palette The flavors range from smooth earthiness and light floral scents to refreshingly sweet and sour notes What they all share is the recollection of how some Ancient One tamed fire and water to coexist harmoniously and steeped in them the gifts of Nature Since that distant past people have soothed roused healed explored and celebrated with these wonderful infusions We encourage you to do the same with Numi s finest The price is perfect a big savings over single boxes If you like Numi teas and want to sample a wonderful assortment get it you won t be sorry There may be some you like and others you don t but sampling them will be a delightful experience If you are new to Numi this is an excellent way to sample their high quality teas I do not think you could find a better source of premium tea than Numi Highly recommended'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.iloc[1][\"review\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Very Good I received this product early from t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Organic Kosher Tasty Assortment of Premium Tea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>excellent gluten free spaghetti great taste gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Lindt is Lindt Buying this multi pack I was mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>YUM These bars are so good I loved them warmed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Delicious I love these chips I buy the 24 pack...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>Tastes great and it s organic I m a huge fan o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>Poor taste I was really disappointed with the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>Better than US Instant Coffee A friend who has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>Hard not to like No need for plastic baggies o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score                                             review\n",
       "0      5  Very Good I received this product early from t...\n",
       "1      5  Organic Kosher Tasty Assortment of Premium Tea...\n",
       "2      5  excellent gluten free spaghetti great taste gr...\n",
       "3      5  Lindt is Lindt Buying this multi pack I was mi...\n",
       "4      5  YUM These bars are so good I loved them warmed...\n",
       "5      5  Delicious I love these chips I buy the 24 pack...\n",
       "6      5  Tastes great and it s organic I m a huge fan o...\n",
       "7      2  Poor taste I was really disappointed with the ...\n",
       "8      3  Better than US Instant Coffee A friend who has...\n",
       "9      4  Hard not to like No need for plastic baggies o..."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.10 transform to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['review'] = final_df['review'].apply(toLower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'organic kosher tasty assortment of premium teas teasans numi s collection assortment melange includes 5 herbal teas caffeine free also called teasans dry desert lime lime herbal teasan fields of gold lemongrass herbal teasan red mellow bush rooibos herbal teasan bushmen s brew honeybush herbal teasan simply mint moroccan mint 2 green teas lower in caffeine temple of heaven gunpowder green tea monkey king jasmine green tea 2 black teas contain caffeine chinese breakfast yunnan black tea smoky tarry lapsang souchong black tea this is a total of nine different teas two tea bags of each one in each box numi teas are known for their high quality organic and kosher ingredients and in my opinion are some of the tastiest and best teas i have ever tried they do not include artificial ingredients or flavorings on the box the manufacturer writes from mist covered mountains to sun drenched deserts to fertile fields we proudly bring you our tea palette the flavors range from smooth earthiness and light floral scents to refreshingly sweet and sour notes what they all share is the recollection of how some ancient one tamed fire and water to coexist harmoniously and steeped in them the gifts of nature since that distant past people have soothed roused healed explored and celebrated with these wonderful infusions we encourage you to do the same with numi s finest the price is perfect a big savings over single boxes if you like numi teas and want to sample a wonderful assortment get it you won t be sorry there may be some you like and others you don t but sampling them will be a delightful experience if you are new to numi this is an excellent way to sample their high quality teas i do not think you could find a better source of premium tea than numi highly recommended'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.iloc[1][\"review\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.11 remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['review'] = final_df['review'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'organic kosher tasty assortment premium teas teasans numi collection assortment melange includes 5 herbal teas caffeine free also called teasans dry desert lime lime herbal teasan fields gold lemongrass herbal teasan red mellow bush rooibos herbal teasan bushmen brew honeybush herbal teasan simply mint moroccan mint 2 green teas lower caffeine temple heaven gunpowder green tea monkey king jasmine green tea 2 black teas contain caffeine chinese breakfast yunnan black tea smoky tarry lapsang souchong black tea total nine different teas two tea bags one box numi teas known high quality organic kosher ingredients opinion tastiest best teas ever tried not include artificial ingredients flavorings box manufacturer writes mist covered mountains sun drenched deserts fertile fields proudly bring tea palette flavors range smooth earthiness light floral scents refreshingly sweet sour notes share recollection ancient one tamed fire water coexist harmoniously steeped gifts nature since distant past people soothed roused healed explored celebrated wonderful infusions encourage numi finest price perfect big savings single boxes like numi teas want sample wonderful assortment get sorry may like others sampling delightful experience new numi excellent way sample high quality teas not think could find better source premium tea numi highly recommended'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.iloc[1][\"review\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.12 remove extra spaces again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['review'] = final_df['review'].apply(remove_extra_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>very good received product early seller tastey...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>organic kosher tasty assortment premium teas t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>excellent gluten free spaghetti great taste gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>lindt lindt buying multi pack misled picture w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>yum bars good loved warmed definitely think gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>delicious love chips buy 24 pack month bags ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>tastes great organic huge fan eating cereal br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>poor taste really disappointed sorghum purchas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>better us instant coffee friend gone korea gav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>hard not like no need plastic baggies sloppy t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score                                             review\n",
       "0      5  very good received product early seller tastey...\n",
       "1      5  organic kosher tasty assortment premium teas t...\n",
       "2      5  excellent gluten free spaghetti great taste gr...\n",
       "3      5  lindt lindt buying multi pack misled picture w...\n",
       "4      5  yum bars good loved warmed definitely think gr...\n",
       "5      5  delicious love chips buy 24 pack month bags ri...\n",
       "6      5  tastes great organic huge fan eating cereal br...\n",
       "7      2  poor taste really disappointed sorghum purchas...\n",
       "8      3  better us instant coffee friend gone korea gav...\n",
       "9      4  hard not like no need plastic baggies sloppy t..."
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data Explore and Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Visualize score count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_score_count(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Describe Score Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">review</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Score</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39193</td>\n",
       "      <td>28452</td>\n",
       "      <td>filler food empty leaves cat always needing re...</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22353</td>\n",
       "      <td>16278</td>\n",
       "      <td>personal preference hoped tea would taste bett...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31993</td>\n",
       "      <td>23276</td>\n",
       "      <td>i pass not enough flavor murky brown color lac...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60309</td>\n",
       "      <td>43837</td>\n",
       "      <td>stash chamomile herbal tea stash chamomile her...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>272492</td>\n",
       "      <td>196967</td>\n",
       "      <td>fantastic food good cat health pet food indust...</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       review                                                                \n",
       "        count  unique                                                top freq\n",
       "Score                                                                        \n",
       "1       39193   28452  filler food empty leaves cat always needing re...  140\n",
       "2       22353   16278  personal preference hoped tea would taste bett...   23\n",
       "3       31993   23276  i pass not enough flavor murky brown color lac...   22\n",
       "4       60309   43837  stash chamomile herbal tea stash chamomile her...   30\n",
       "5      272492  196967  fantastic food good cat health pet food indust...   37"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.groupby('Score').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 Analyze finding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we found that not all are unique. so we investigate further for uniqness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0 calories 0 fat 0 carbs 0 sugar sweeeet torani sugar free vanilla flavor syrup nice way add little variety coffee without run starbuck sugar free vanilla latte mentioned title no calories fats carbs sugars taste sweet vanillery use anything want add little variety flavor like pellegrino makes nice vanilla soda coffee found not add anything else syrup good enough tasty enough use whenever get sweet tooth get past craving something calories involved sweetened splenda not used using sugar substitutes may not like little bit taste leaves ingrediants listed water vanilla extract natural flavors potassium sorbate sodium benzoate 5 mg sodium listed label citric acid xanthan gum sucralose splenda acesulfame potassium bargain 750 ml sold paid almost much 375ml local grocery store probably use two tablespoons per drink less get money worth add anything want hot chocolate iced tea etc maybe holiday coming would nice change beverages well also sold sugar added several flavors without sugar click products torani top page cheers laurie</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 calories awesome prepared awful smell opening bag based reviews water smells like fish not big deal rinsed noodles 2 minutes hot water placed boiling water another 2 3 minutes mixed noodles sauce popular spicy szechuan style vegetable chicken weight loss frozen food softened microwave cut sauce noodles packaged put noodles sauce fridge 22 hours eating based suggestion reviews texture strange no food product market 100 compare rubbery somewhat hard break teeth chew marinating noodles not help still flavorless matter much sauce spicy enough mask fact noodles no flavor suggest noodles anyone trying lose weight cut carbs not something would want eat otherwise</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 calories passable taste watching calorie intake love product no calories huge plus knew taste big full calorie full fat peanut butters tried pb j tasted fine local store would carry product instead order online</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 good made panna cotta one main ingredients gelatin came great would recommend even though going try products</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 net carbs delicious bought 8 oz cocoa local co op impressed every brand cocoa bought added sugar cocoa not 2 carbs 2 grams fiber per tablespoon strong genuine chocolate flavor use make chocolate mousse hot cocoa add chocolate flavor baked goods not able find stores since relocated pleased see amazon highly recommended information available company site http www equalexchange coop cocoa</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 product given brother use days said one defiantly like smell likes two one danruff dead skin scalp areas notice gone although not everything days still continue use likes way hair feels dandruff dead skin may help symptoms knows lets see month two</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 proteins got product nursing milk proteins difficult digest baby babies vomit spit early months daughter stopped vomiting discontinued milk products milk almond based no proteins tastes great too</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 star could totally agreed dowd comment review green tea tasted horrible bitter bought lot pretty good whole leaf tea china drank lot ten ren tea bay area ambassador organics green tea nothing close throwing away one cup comparison anything get saveway better also one tin boxes came empty totally empty tin dented believe either used open product problem wearhouse glad amazon resolved issue thanks</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 stars actual rating not purchase keurig thought purchasing community coffee k cup keurig search amazon search line received found filter bags coffee spent 60 6 boxes big mistake part notice called pods thought different term k cup another big mistake part not work keurig coffee comes look slightly dirty water no flavor whatsoever tried many ways brewing like used k cup plastic lid made wetting bag brewing nothing worked water flowed pod like water instead steeping like coffee keurig owners save money</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 stars could yuk yuk yuk immense love pancakes search best pancake mix year not write review prematurely wanted give product fair chance tried whole milk tried buttermilk tried blueberries tried without blueberries far worst mix tried thus far flavor not good seems like needs sugar blueberries hard get pancakes cook way without overcooking really telling hype product got people either work company lol never tried another high quality mix honestly would rather eat pancakes mcdonalds waste money robbys mix also wayyyyyyyyyy better believe cheaper may better waffles since stuck two cans may try ugh</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 stars option cute really wanted work new potato bag orka first time tried stick something without anything even hanging it fell hit floor cheap plastic hook broke hook looks metal silver colored plastic ordered 2 sent right back immediately</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 stars wrong product negative star rating ordered cashew butter got macadamia butter allergic no contact info want cashew butter</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 week like product researching online different products decided brand take twice day brew mines cold ice tea notice everything get good cleanse drinking tea regular also notice different metabolism</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00 superfine super antimo caputo superfine farina flour great product flour perfect pizzas foccacias makes crumb fine tasty hard believe finer grind makes much difference love it</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0610 gopher trap set trap day received gopher pushed dirt onto also pushed trap burrow reset trap nail behind keep place two hours later gopher problem solved</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09 23 11 prompt delivery no damage product great job tea perfect sip needing moment recharge batteries need feel warm cozy</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0g trans fat made organic corn eat various dips find good many flavors dips baked chips crispy heavily seasoned seasoning awesome cooks skimp guiltless gourmet excellent guests splurging healthy eats taste better processed also recommend anyone eats tortilla chips black bean chips blow competition away</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1 00 meal charts let start saying dish incredible periodically order jon pong favorite chinese resturant called china express located rtp nc run jane charlie although not know last names order korean style means instead haing blistering hot red peppers floatng aroung somehow blend tha seasoning broth comes red served always make ensure order correct guess always no squid no pork however real shrimp real scallops difference jon pong costs almost 8 00 bowl cost 1 20 bowl one good take pick sincerely james w fowler</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1 00 per liter buy nuts get pepsi coke etc sale less 1 00 per 2 liter twice much plus cost machine carbonate total waste money</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1 1 1 neutral might remember last review clear scalp hair beauty volumizing root boost nourishing shampoo 12 9 fluid ounce wife went absolutely nutes shampoo well noted version men urged order pretty folically challenged much shampoo hair feels good works fine use really suspect wanted ordered use stuff however sons still full head hair used youngest curley hair likes says seeing flakes since used oldest hair like mine hair say dries hair loves clear scalp hair beauty volumizing root boost nourishing shampoo 12 9 fluid ounce mixed review 1 strong vote one one neutral also appears enough variety line find product line right you</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1 1 10 good initially learned product dr oz oprah show love love oatmeal great sugar substitute</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1 10 strong brands disolve hot coffee loved using nustevia 5 years tried using stevia instead dissolve hot coffe takes 9 packets brand instead 2 packets brands customer service refuses pay ship back not waste money diluted non disolving stevia cheaper buy concentrated brand disolve coffee</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1 11 go bought 12 poack price good far eaten 1 11 go ingredients solid easy read still deciding taste chia fine seed little different taste thats packed nutrients product edible 11 bags sure either sick craving it</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1 18 grams 0 75 grams xylitol 2011 1 08 grams 0 72 grams xylitol competitive xyloburst 100 xylitol gum 1 5 grams 1 0 grams xylitol sweetener gum apparently 100 xylitol opposed trident sugarless xylitol not find explicit indication probably far less xylitol perhaps much aspartame seems simply holding 1 2 teaspoon xylitol mouth long possible leads far higher peak concentration xylitol saturate bacteria holding far longer chewing gum stop swallowing chewing small chewing gum peak concentration xylitol far lower mouth gum</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1 2 1 2 cereal delicious box last v long less week not sure getting money worth</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1 2 1 2 coffee perfect decaf regular coffee popular friends good item ready serve guests</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1 2 case not pick fine print case dropped 8 loves 4 certainly accounts good price instead saving 15 bucks paid 15 bucks extra</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1 2 cats recommend cnc two cats snack motivated third not vote not included tried salmon chicken crunch n clean bit pricey side anyone ever tried brush cat teeth tell anything lend hand kitty dental hygiene worth cats seem preference chicken salmon chased gobbled treats immediately problem though whether size texture treats individual cat one two cats size generally disposition bothered chew swallowed whole mind giving cats treats prefer brands junk food case one getting possibility dental benefit</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1 2 clams sand shell fragments product mainland china advantages 1 clams taste quite good 2 easy open opener 3 oysters extremely extremely rich source zinc clams extremely extremely rich source iron 4 clams low mercury marine pollutants 5 clams contain small amount long chain omega 3 fatty acids epa dha shown prevent many human diseases 6 less 2 00 per price right 7 quantity clams adequately good disadvantages 1 approximately 1 2 clams sand attached shell fragments</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1 2 dogs liked length 1 42 mins since neither 2 dogs write new lickety stik best new idea treats always welcomed 3 flavors stiks bacon chicken liver tried three sundae casey sundae licked three seemed like casey however try three turning head apparent disgust lol dogs whole life never tell dog like dislike give 5 stars functional reasons not casey even try first lick ball exactly like deodorant ball matter fact container reminds arrid rollerball deodorant bottle exactly move freely licking gently squeeze bottle liquid available dog squeeze hard drips onto floor seems enjoyable supposed healthy use magnifying glass read rather small label video one taste tests hope enjoy it</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zukes dogs love great treat give dog make happy great price get free shipping</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zukes good enticing product added benefit low calorie dog loves dogs dogpark love too</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zukes great zuke training treats made excellent ingredients none crap eat small size allows give golden bigger size</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zukes hip action buying cosequin vet slightly overweight ok obese cat jackson cost 45 per month helpful years recently begun limping age 14 learned zukes hip action decided try fun jackson jumping furniture no longer limps silky smooth coat 6 weeks costs 1 10 much jackson loves strongly recommended</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zukes hip action treats boys hips getting worse old age every little bit helps dog loves like made usa</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zukes hip action treats dogs love helps keep joints limber dachshund fussy loves glucosomine treats eat ones vets fussy older dog needs please try happy would recommend smaller bag try upgrade 3 packs zukes hip action jerky naturals beef 3lb</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zukes hip action treats great eats nino shiba ino became 14 nino regressing severe case arthritis rear legs two years ago placed diet ark naturals sea mobility beef jerky sea cucumber msm glucosamine supplement dogs within month showed significant improvement go one mile walks morning walks gracefully without apparent ailment since special advertised amazon 3 packs zukes hip action jerky naturals beef product highly rated decided buy nino treats not zukes hip action jerky naturals preventing crippling arthritis rear legs looks forward treat morning walk</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zukes hipaction dogs works 12 year old corgi problems going stairs no interest playing toys last year went vet gave rimadyl helped still issues carry stairs started giving zukes whim seeing local pet store stuff works sold playing dogs goes stairs brings toys us play like old self again</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zukes love products love idea behind fact help animals cancer zukes president please</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zukes mini naturals dog treats treats tiny great training low calorie yet aromatic dog jumps joy smells customer service great except back ordered one package treats assured weeks waiting month item</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zukes mini naturals healthy treats toy poodle loves treats use training maintaining bathroom breaks outside use smaller treated pee rewards larger ones another brand poo rewards first thought would excessive tried stop set back accidents house felt rewards far better price pay messes house punishing dog way everyone happy since give rewards several times daily felt natural treats far better choice trained dog lay eating no begging roll sit stay using treats highly recommend</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zukes mini naturals training kaylee 75 lb german shepherd found not like anything treats food large chunks dainty eater even smallest milk bones dog biscuits big liking neice gave bag zukes mini naturals 1st birthday went crazy perfect training responds wonderfully highly recommend dog dainty eater never one even swallow hot dog whole like last dog lab mini naturals loved bob</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zukes mini treats hi love brand zukes chicken mini treats dog love natural healthy dog</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zukes no wrong loyal fan zukes right mind set comes dogs need treated fed love company precious little angel starlo pit boxer mix dearly misses cranberry z ridge bones favorite anything bought zukes well worth money spent keep great work zukes side note work grooming salon kennel generic dog bones treats keep bag salmon training treats near proven resist succulent moist flavor compared dry bland bones</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zukes peanut butter mini treats product dog absolute favorite comes dog treats find perfect training she</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zukes treats dog loves treats whatever flavor may favorites healthy zuke chicken flavored mini naturals dog treats 16 ounces</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zukes treats dogs love treats small bag lasts long time great value price</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zukes z bone giant edible dental chew clean apple crisp 5 ounce bone dog likes somewhat odor bone hold horrible place plastic bag guess since human not eating must smell guess worth price biggest issue bone expensive adding shipping expensive think alternatives better</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zulka sugar tried bag sugar less expensive brands sugar highly refined good higher priced brands could not find locally bought amazon good price satisfied purchase</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zuma loves treats zuma hairless chinese crested pup finicky eater would rather starve eat something like food eat cesar gourmet filets sauce ok find dog treat likes husband brought home cesar grilled chicken softies dog treats loves treats look smell like dog treat thought no way zuma would eat hundreds dollars spent numerous bog bakeries year half spent shopping all place found cesar grilled chicken softies dog treats not thrilled relieved</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zw farm year old aussie loves rawhides takes really long time get one heavy chewer would recommend anyone larger size dog loves chew</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zwieback light slightly sweet crunchy biscuit bread tastes great butter put anything it light also great upset stomachs etc eat breakfast every morning like brandt best</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zwieback toast zwieback toast good taste compare old nabisco zwieback quite measure also not like size product small comparison zwieback products k would not order again</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zylitol healthy great taste loved preserves got strawberry blueberry great taste good know made healthy xylitol said sweetener not feed candida thank nature hallow</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zylotol products cause instestinal problems far taste factor goes really good substitute table sugar aftertaste stevia based also plant products however cause diarreha people metformin low borderline diabetes causes diarreha 50 people well started using product cancel appointments stay home 24 hours made matters worse believe someone else commented diarreha problems zylotol notice bought tried product</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zz plant awesome house plant everyone must one interesting look fast growing easy take care of</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zzfantastic coffee luxury experiencing blue mountain kona little farm beans obscure travels friends coffee ranks right waited review drinking month 4 weeks of daily brewing still takes great seems like low acidity unique flavor not lend normal categories cherries chocolate etc price point not bother 30 dollar delivery charge seems range good</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zzzzzzzzz product amazing tried dream water nuero sleep dream water worked time no matter would knocked half hour</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zzzzzzzzz stuff awesome problems anxiety insomnia one cup really relaxes bag good least two cups lot chamomile seen traditional american mexican chamomile tea bags also tastes better chamomile teas tried always house</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zzzzzzzzzzz snort received 1 45 oz package drops evaluate not exactly enough cause sugar high nine drops three daily requirement attest taste alright like cheap candy first ingredient sugar second corn syrup know true read b12 effective injection sub lingual dissolving little pills tongue opinion better advised buy trusted brand mufti vitamins bag caramels crave like taking pills could try gummy candy type vitamins</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>308783 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Score\n",
       "review                                                   \n",
       "0 calories 0 fat 0 carbs 0 sugar sweeeet torani...      1\n",
       "0 calories awesome prepared awful smell opening...      1\n",
       "0 calories passable taste watching calorie inta...      1\n",
       "0 good made panna cotta one main ingredients ge...      1\n",
       "0 net carbs delicious bought 8 oz cocoa local c...      1\n",
       "0 product given brother use days said one defia...      1\n",
       "0 proteins got product nursing milk proteins di...      1\n",
       "0 star could totally agreed dowd comment review...      1\n",
       "0 stars actual rating not purchase keurig thoug...      1\n",
       "0 stars could yuk yuk yuk immense love pancakes...      1\n",
       "0 stars option cute really wanted work new pota...      1\n",
       "0 stars wrong product negative star rating orde...      1\n",
       "0 week like product researching online differen...      1\n",
       "00 superfine super antimo caputo superfine fari...      1\n",
       "0610 gopher trap set trap day received gopher p...      1\n",
       "09 23 11 prompt delivery no damage product grea...      1\n",
       "0g trans fat made organic corn eat various dips...      1\n",
       "1 00 meal charts let start saying dish incredib...      1\n",
       "1 00 per liter buy nuts get pepsi coke etc sale...      1\n",
       "1 1 1 neutral might remember last review clear ...      1\n",
       "1 1 10 good initially learned product dr oz opr...      1\n",
       "1 10 strong brands disolve hot coffee loved usi...      5\n",
       "1 11 go bought 12 poack price good far eaten 1 ...      1\n",
       "1 18 grams 0 75 grams xylitol 2011 1 08 grams 0...      1\n",
       "1 2 1 2 cereal delicious box last v long less w...      1\n",
       "1 2 1 2 coffee perfect decaf regular coffee pop...      2\n",
       "1 2 case not pick fine print case dropped 8 lov...      1\n",
       "1 2 cats recommend cnc two cats snack motivated...      3\n",
       "1 2 clams sand shell fragments product mainland...      1\n",
       "1 2 dogs liked length 1 42 mins since neither 2...      5\n",
       "...                                                   ...\n",
       "zukes dogs love great treat give dog make happy...      4\n",
       "zukes good enticing product added benefit low c...      4\n",
       "zukes great zuke training treats made excellent...      4\n",
       "zukes hip action buying cosequin vet slightly o...      2\n",
       "zukes hip action treats boys hips getting worse...      5\n",
       "zukes hip action treats dogs love helps keep jo...      1\n",
       "zukes hip action treats great eats nino shiba i...      1\n",
       "zukes hipaction dogs works 12 year old corgi pr...      3\n",
       "zukes love products love idea behind fact help ...      5\n",
       "zukes mini naturals dog treats treats tiny grea...      5\n",
       "zukes mini naturals healthy treats toy poodle l...      5\n",
       "zukes mini naturals training kaylee 75 lb germa...      5\n",
       "zukes mini treats hi love brand zukes chicken m...      5\n",
       "zukes no wrong loyal fan zukes right mind set c...      1\n",
       "zukes peanut butter mini treats product dog abs...      5\n",
       "zukes treats dog loves treats whatever flavor m...      5\n",
       "zukes treats dogs love treats small bag lasts l...      4\n",
       "zukes z bone giant edible dental chew clean app...      1\n",
       "zulka sugar tried bag sugar less expensive bran...      1\n",
       "zuma loves treats zuma hairless chinese crested...      1\n",
       "zw farm year old aussie loves rawhides takes re...      1\n",
       "zwieback light slightly sweet crunchy biscuit b...      1\n",
       "zwieback toast zwieback toast good taste compar...      1\n",
       "zylitol healthy great taste loved preserves got...      1\n",
       "zylotol products cause instestinal problems far...      1\n",
       "zz plant awesome house plant everyone must one ...      1\n",
       "zzfantastic coffee luxury experiencing blue mou...      1\n",
       "zzzzzzzzz product amazing tried dream water nue...      2\n",
       "zzzzzzzzz stuff awesome problems anxiety insomn...      1\n",
       "zzzzzzzzzzz snort received 1 45 oz package drop...      1\n",
       "\n",
       "[308783 rows x 1 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.groupby('review').agg('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualize one of the duplicate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = final_df['review'].str.find('I only used two maybe three tea bags and got pregnan')\n",
    "t = t[t != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: review, dtype: int64)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i used two maybe three tea bags got pregnant not drink pregnancy not bad taste not big tea fan either'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.loc[43935]['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i used two maybe three tea bags got pregnant not drink pregnancy not bad taste not big tea fan either'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.loc[187578]['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i used two maybe three tea bags got pregnant not drink pregnancy not bad taste not big tea fan either'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.loc[306678]['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check initial row count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Score     426340\n",
       "review    426340\n",
       "dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4 Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.drop_duplicates(keep=\"first\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check new row count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Score     308810\n",
       "review    308810\n",
       "dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">review</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Score</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28452</td>\n",
       "      <td>28452</td>\n",
       "      <td>not organic unfortunately assumed bob red mill...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16278</td>\n",
       "      <td>16278</td>\n",
       "      <td>disappointed taste trying find higher protein ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23276</td>\n",
       "      <td>23276</td>\n",
       "      <td>a little disappointed little disappointed part...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43837</td>\n",
       "      <td>43837</td>\n",
       "      <td>lucini extra virgin olive oil great olive oil ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>196967</td>\n",
       "      <td>196967</td>\n",
       "      <td>best vegan protein far use lot protein powder ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       review                                                                \n",
       "        count  unique                                                top freq\n",
       "Score                                                                        \n",
       "1       28452   28452  not organic unfortunately assumed bob red mill...    1\n",
       "2       16278   16278  disappointed taste trying find higher protein ...    1\n",
       "3       23276   23276  a little disappointed little disappointed part...    1\n",
       "4       43837   43837  lucini extra virgin olive oil great olive oil ...    1\n",
       "5      196967  196967  best vegan protein far use lot protein powder ...    1"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.groupby('Score').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>very good received product early seller tastey...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>organic kosher tasty assortment premium teas t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>excellent gluten free spaghetti great taste gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>lindt lindt buying multi pack misled picture w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>yum bars good loved warmed definitely think gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>delicious love chips buy 24 pack month bags ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>tastes great organic huge fan eating cereal br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>poor taste really disappointed sorghum purchas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>better us instant coffee friend gone korea gav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>hard not like no need plastic baggies sloppy t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score                                             review\n",
       "0      5  very good received product early seller tastey...\n",
       "1      5  organic kosher tasty assortment premium teas t...\n",
       "2      5  excellent gluten free spaghetti great taste gr...\n",
       "3      5  lindt lindt buying multi pack misled picture w...\n",
       "4      5  yum bars good loved warmed definitely think gr...\n",
       "5      5  delicious love chips buy 24 pack month bags ri...\n",
       "6      5  tastes great organic huge fan eating cereal br...\n",
       "7      2  poor taste really disappointed sorghum purchas...\n",
       "8      3  better us instant coffee friend gone korea gav...\n",
       "9      4  hard not like no need plastic baggies sloppy t..."
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good not amazing sauce mild expecting something much hotter literally pour food without difficulty taste good extremely sweet go lot foods someone gave second bottle gift certainly use buy second bottle either like sweet sauces looking scorching addition food good choice'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.iloc[124]['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Text Processing and Normalization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Extract part of final data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_sub = final_df.head(10).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>very good received product early seller tastey...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>organic kosher tasty assortment premium teas t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>excellent gluten free spaghetti great taste gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>lindt lindt buying multi pack misled picture w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>yum bars good loved warmed definitely think gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>delicious love chips buy 24 pack month bags ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>tastes great organic huge fan eating cereal br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>poor taste really disappointed sorghum purchas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>better us instant coffee friend gone korea gav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>hard not like no need plastic baggies sloppy t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score                                             review\n",
       "0      5  very good received product early seller tastey...\n",
       "1      5  organic kosher tasty assortment premium teas t...\n",
       "2      5  excellent gluten free spaghetti great taste gr...\n",
       "3      5  lindt lindt buying multi pack misled picture w...\n",
       "4      5  yum bars good loved warmed definitely think gr...\n",
       "5      5  delicious love chips buy 24 pack month bags ri...\n",
       "6      5  tastes great organic huge fan eating cereal br...\n",
       "7      2  poor taste really disappointed sorghum purchas...\n",
       "8      3  better us instant coffee friend gone korea gav...\n",
       "9      4  hard not like no need plastic baggies sloppy t..."
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_sub.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_sub['token'] = final_df_sub['review'].apply(tokeniz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>review</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>very good received product early seller tastey...</td>\n",
       "      <td>[very, good, received, product, early, seller,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>organic kosher tasty assortment premium teas t...</td>\n",
       "      <td>[organic, kosher, tasty, assortment, premium, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>excellent gluten free spaghetti great taste gr...</td>\n",
       "      <td>[excellent, gluten, free, spaghetti, great, ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>lindt lindt buying multi pack misled picture w...</td>\n",
       "      <td>[lindt, lindt, buying, multi, pack, misled, pi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>yum bars good loved warmed definitely think gr...</td>\n",
       "      <td>[yum, bars, good, loved, warmed, definitely, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>delicious love chips buy 24 pack month bags ri...</td>\n",
       "      <td>[delicious, love, chips, buy, 24, pack, month,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>tastes great organic huge fan eating cereal br...</td>\n",
       "      <td>[tastes, great, organic, huge, fan, eating, ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>poor taste really disappointed sorghum purchas...</td>\n",
       "      <td>[poor, taste, really, disappointed, sorghum, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>better us instant coffee friend gone korea gav...</td>\n",
       "      <td>[better, us, instant, coffee, friend, gone, ko...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>hard not like no need plastic baggies sloppy t...</td>\n",
       "      <td>[hard, not, like, no, need, plastic, baggies, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score                                             review  \\\n",
       "0      5  very good received product early seller tastey...   \n",
       "1      5  organic kosher tasty assortment premium teas t...   \n",
       "2      5  excellent gluten free spaghetti great taste gr...   \n",
       "3      5  lindt lindt buying multi pack misled picture w...   \n",
       "4      5  yum bars good loved warmed definitely think gr...   \n",
       "5      5  delicious love chips buy 24 pack month bags ri...   \n",
       "6      5  tastes great organic huge fan eating cereal br...   \n",
       "7      2  poor taste really disappointed sorghum purchas...   \n",
       "8      3  better us instant coffee friend gone korea gav...   \n",
       "9      4  hard not like no need plastic baggies sloppy t...   \n",
       "\n",
       "                                               token  \n",
       "0  [very, good, received, product, early, seller,...  \n",
       "1  [organic, kosher, tasty, assortment, premium, ...  \n",
       "2  [excellent, gluten, free, spaghetti, great, ta...  \n",
       "3  [lindt, lindt, buying, multi, pack, misled, pi...  \n",
       "4  [yum, bars, good, loved, warmed, definitely, t...  \n",
       "5  [delicious, love, chips, buy, 24, pack, month,...  \n",
       "6  [tastes, great, organic, huge, fan, eating, ce...  \n",
       "7  [poor, taste, really, disappointed, sorghum, p...  \n",
       "8  [better, us, instant, coffee, friend, gone, ko...  \n",
       "9  [hard, not, like, no, need, plastic, baggies, ...  "
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_sub.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Stemming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.01101231575012207 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Stem data and calculate time needed for that\n",
    "start_time = time.time()\n",
    "final_df_sub['stem'] = final_df_sub['review'].apply(stemm)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>review</th>\n",
       "      <th>token</th>\n",
       "      <th>stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>very good received product early seller tastey...</td>\n",
       "      <td>[very, good, received, product, early, seller,...</td>\n",
       "      <td>veri good receiv product earli seller tastey g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>organic kosher tasty assortment premium teas t...</td>\n",
       "      <td>[organic, kosher, tasty, assortment, premium, ...</td>\n",
       "      <td>organ kosher tasti assort premium tea teasan n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>excellent gluten free spaghetti great taste gr...</td>\n",
       "      <td>[excellent, gluten, free, spaghetti, great, ta...</td>\n",
       "      <td>excel gluten free spaghetti great tast great s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>lindt lindt buying multi pack misled picture w...</td>\n",
       "      <td>[lindt, lindt, buying, multi, pack, misled, pi...</td>\n",
       "      <td>lindt lindt buy multi pack misl pictur whole h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>yum bars good loved warmed definitely think gr...</td>\n",
       "      <td>[yum, bars, good, loved, warmed, definitely, t...</td>\n",
       "      <td>yum bar good love warm definit think great sna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>delicious love chips buy 24 pack month bags ri...</td>\n",
       "      <td>[delicious, love, chips, buy, 24, pack, month,...</td>\n",
       "      <td>delici love chip buy 24 pack month bag right s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>tastes great organic huge fan eating cereal br...</td>\n",
       "      <td>[tastes, great, organic, huge, fan, eating, ce...</td>\n",
       "      <td>tast great organ huge fan eat cereal breakfast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>poor taste really disappointed sorghum purchas...</td>\n",
       "      <td>[poor, taste, really, disappointed, sorghum, p...</td>\n",
       "      <td>poor tast realli disappoint sorghum purchas us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>better us instant coffee friend gone korea gav...</td>\n",
       "      <td>[better, us, instant, coffee, friend, gone, ko...</td>\n",
       "      <td>better us instant coffe friend gone korea gave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>hard not like no need plastic baggies sloppy t...</td>\n",
       "      <td>[hard, not, like, no, need, plastic, baggies, ...</td>\n",
       "      <td>hard not like no need plastic baggi sloppi tin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score                                             review  \\\n",
       "0      5  very good received product early seller tastey...   \n",
       "1      5  organic kosher tasty assortment premium teas t...   \n",
       "2      5  excellent gluten free spaghetti great taste gr...   \n",
       "3      5  lindt lindt buying multi pack misled picture w...   \n",
       "4      5  yum bars good loved warmed definitely think gr...   \n",
       "5      5  delicious love chips buy 24 pack month bags ri...   \n",
       "6      5  tastes great organic huge fan eating cereal br...   \n",
       "7      2  poor taste really disappointed sorghum purchas...   \n",
       "8      3  better us instant coffee friend gone korea gav...   \n",
       "9      4  hard not like no need plastic baggies sloppy t...   \n",
       "\n",
       "                                               token  \\\n",
       "0  [very, good, received, product, early, seller,...   \n",
       "1  [organic, kosher, tasty, assortment, premium, ...   \n",
       "2  [excellent, gluten, free, spaghetti, great, ta...   \n",
       "3  [lindt, lindt, buying, multi, pack, misled, pi...   \n",
       "4  [yum, bars, good, loved, warmed, definitely, t...   \n",
       "5  [delicious, love, chips, buy, 24, pack, month,...   \n",
       "6  [tastes, great, organic, huge, fan, eating, ce...   \n",
       "7  [poor, taste, really, disappointed, sorghum, p...   \n",
       "8  [better, us, instant, coffee, friend, gone, ko...   \n",
       "9  [hard, not, like, no, need, plastic, baggies, ...   \n",
       "\n",
       "                                                stem  \n",
       "0  veri good receiv product earli seller tastey g...  \n",
       "1  organ kosher tasti assort premium tea teasan n...  \n",
       "2  excel gluten free spaghetti great tast great s...  \n",
       "3  lindt lindt buy multi pack misl pictur whole h...  \n",
       "4  yum bar good love warm definit think great sna...  \n",
       "5  delici love chip buy 24 pack month bag right s...  \n",
       "6  tast great organ huge fan eat cereal breakfast...  \n",
       "7  poor tast realli disappoint sorghum purchas us...  \n",
       "8  better us instant coffe friend gone korea gave...  \n",
       "9  hard not like no need plastic baggi sloppi tin...  "
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_sub.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Lemmatizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.0029916763305664062 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize data and calculate time needed for that\n",
    "start_time = time.time()\n",
    "final_df_sub['lemm'] = final_df_sub['review'].apply(lemm)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>review</th>\n",
       "      <th>token</th>\n",
       "      <th>stem</th>\n",
       "      <th>lemm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>very good received product early seller tastey...</td>\n",
       "      <td>[very, good, received, product, early, seller,...</td>\n",
       "      <td>veri good receiv product earli seller tastey g...</td>\n",
       "      <td>very good received product early seller tastey...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>organic kosher tasty assortment premium teas t...</td>\n",
       "      <td>[organic, kosher, tasty, assortment, premium, ...</td>\n",
       "      <td>organ kosher tasti assort premium tea teasan n...</td>\n",
       "      <td>organic kosher tasty assortment premium tea te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>excellent gluten free spaghetti great taste gr...</td>\n",
       "      <td>[excellent, gluten, free, spaghetti, great, ta...</td>\n",
       "      <td>excel gluten free spaghetti great tast great s...</td>\n",
       "      <td>excellent gluten free spaghetti great taste gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>lindt lindt buying multi pack misled picture w...</td>\n",
       "      <td>[lindt, lindt, buying, multi, pack, misled, pi...</td>\n",
       "      <td>lindt lindt buy multi pack misl pictur whole h...</td>\n",
       "      <td>lindt lindt buying multi pack misled picture w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>yum bars good loved warmed definitely think gr...</td>\n",
       "      <td>[yum, bars, good, loved, warmed, definitely, t...</td>\n",
       "      <td>yum bar good love warm definit think great sna...</td>\n",
       "      <td>yum bar good loved warmed definitely think gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>delicious love chips buy 24 pack month bags ri...</td>\n",
       "      <td>[delicious, love, chips, buy, 24, pack, month,...</td>\n",
       "      <td>delici love chip buy 24 pack month bag right s...</td>\n",
       "      <td>delicious love chip buy 24 pack month bag righ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>tastes great organic huge fan eating cereal br...</td>\n",
       "      <td>[tastes, great, organic, huge, fan, eating, ce...</td>\n",
       "      <td>tast great organ huge fan eat cereal breakfast...</td>\n",
       "      <td>taste great organic huge fan eating cereal bre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>poor taste really disappointed sorghum purchas...</td>\n",
       "      <td>[poor, taste, really, disappointed, sorghum, p...</td>\n",
       "      <td>poor tast realli disappoint sorghum purchas us...</td>\n",
       "      <td>poor taste really disappointed sorghum purchas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>better us instant coffee friend gone korea gav...</td>\n",
       "      <td>[better, us, instant, coffee, friend, gone, ko...</td>\n",
       "      <td>better us instant coffe friend gone korea gave...</td>\n",
       "      <td>better u instant coffee friend gone korea gave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>hard not like no need plastic baggies sloppy t...</td>\n",
       "      <td>[hard, not, like, no, need, plastic, baggies, ...</td>\n",
       "      <td>hard not like no need plastic baggi sloppi tin...</td>\n",
       "      <td>hard not like no need plastic baggies sloppy t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score                                             review  \\\n",
       "0      5  very good received product early seller tastey...   \n",
       "1      5  organic kosher tasty assortment premium teas t...   \n",
       "2      5  excellent gluten free spaghetti great taste gr...   \n",
       "3      5  lindt lindt buying multi pack misled picture w...   \n",
       "4      5  yum bars good loved warmed definitely think gr...   \n",
       "5      5  delicious love chips buy 24 pack month bags ri...   \n",
       "6      5  tastes great organic huge fan eating cereal br...   \n",
       "7      2  poor taste really disappointed sorghum purchas...   \n",
       "8      3  better us instant coffee friend gone korea gav...   \n",
       "9      4  hard not like no need plastic baggies sloppy t...   \n",
       "\n",
       "                                               token  \\\n",
       "0  [very, good, received, product, early, seller,...   \n",
       "1  [organic, kosher, tasty, assortment, premium, ...   \n",
       "2  [excellent, gluten, free, spaghetti, great, ta...   \n",
       "3  [lindt, lindt, buying, multi, pack, misled, pi...   \n",
       "4  [yum, bars, good, loved, warmed, definitely, t...   \n",
       "5  [delicious, love, chips, buy, 24, pack, month,...   \n",
       "6  [tastes, great, organic, huge, fan, eating, ce...   \n",
       "7  [poor, taste, really, disappointed, sorghum, p...   \n",
       "8  [better, us, instant, coffee, friend, gone, ko...   \n",
       "9  [hard, not, like, no, need, plastic, baggies, ...   \n",
       "\n",
       "                                                stem  \\\n",
       "0  veri good receiv product earli seller tastey g...   \n",
       "1  organ kosher tasti assort premium tea teasan n...   \n",
       "2  excel gluten free spaghetti great tast great s...   \n",
       "3  lindt lindt buy multi pack misl pictur whole h...   \n",
       "4  yum bar good love warm definit think great sna...   \n",
       "5  delici love chip buy 24 pack month bag right s...   \n",
       "6  tast great organ huge fan eat cereal breakfast...   \n",
       "7  poor tast realli disappoint sorghum purchas us...   \n",
       "8  better us instant coffe friend gone korea gave...   \n",
       "9  hard not like no need plastic baggi sloppi tin...   \n",
       "\n",
       "                                                lemm  \n",
       "0  very good received product early seller tastey...  \n",
       "1  organic kosher tasty assortment premium tea te...  \n",
       "2  excellent gluten free spaghetti great taste gr...  \n",
       "3  lindt lindt buying multi pack misled picture w...  \n",
       "4  yum bar good loved warmed definitely think gre...  \n",
       "5  delicious love chip buy 24 pack month bag righ...  \n",
       "6  taste great organic huge fan eating cereal bre...  \n",
       "7  poor taste really disappointed sorghum purchas...  \n",
       "8  better u instant coffee friend gone korea gave...  \n",
       "9  hard not like no need plastic baggies sloppy t...  "
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_sub.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Lemmatizing Data using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 4.68949818611145 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize data and calculate time needed for that\n",
    "start_time = time.time()\n",
    "final_df_sub['spacy_lemm'] = final_df_sub['review'].apply(spacy_lemm)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>review</th>\n",
       "      <th>token</th>\n",
       "      <th>stem</th>\n",
       "      <th>lemm</th>\n",
       "      <th>spacy_lemm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>very good received product early seller tastey...</td>\n",
       "      <td>[very, good, received, product, early, seller,...</td>\n",
       "      <td>veri good receiv product earli seller tastey g...</td>\n",
       "      <td>very good received product early seller tastey...</td>\n",
       "      <td>very good receive product early seller tastey ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>organic kosher tasty assortment premium teas t...</td>\n",
       "      <td>[organic, kosher, tasty, assortment, premium, ...</td>\n",
       "      <td>organ kosher tasti assort premium tea teasan n...</td>\n",
       "      <td>organic kosher tasty assortment premium tea te...</td>\n",
       "      <td>organic kosher tasty assortment premium tea te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>excellent gluten free spaghetti great taste gr...</td>\n",
       "      <td>[excellent, gluten, free, spaghetti, great, ta...</td>\n",
       "      <td>excel gluten free spaghetti great tast great s...</td>\n",
       "      <td>excellent gluten free spaghetti great taste gr...</td>\n",
       "      <td>excellent gluten free spaghetti great taste gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>lindt lindt buying multi pack misled picture w...</td>\n",
       "      <td>[lindt, lindt, buying, multi, pack, misled, pi...</td>\n",
       "      <td>lindt lindt buy multi pack misl pictur whole h...</td>\n",
       "      <td>lindt lindt buying multi pack misled picture w...</td>\n",
       "      <td>lindt lindt buying multi pack mislead picture ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>yum bars good loved warmed definitely think gr...</td>\n",
       "      <td>[yum, bars, good, loved, warmed, definitely, t...</td>\n",
       "      <td>yum bar good love warm definit think great sna...</td>\n",
       "      <td>yum bar good loved warmed definitely think gre...</td>\n",
       "      <td>yum bar good love warmed definitely think grea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>delicious love chips buy 24 pack month bags ri...</td>\n",
       "      <td>[delicious, love, chips, buy, 24, pack, month,...</td>\n",
       "      <td>delici love chip buy 24 pack month bag right s...</td>\n",
       "      <td>delicious love chip buy 24 pack month bag righ...</td>\n",
       "      <td>delicious love chip buy 24 pack month bag righ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>tastes great organic huge fan eating cereal br...</td>\n",
       "      <td>[tastes, great, organic, huge, fan, eating, ce...</td>\n",
       "      <td>tast great organ huge fan eat cereal breakfast...</td>\n",
       "      <td>taste great organic huge fan eating cereal bre...</td>\n",
       "      <td>taste great organic huge fan eat cereal breakf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>poor taste really disappointed sorghum purchas...</td>\n",
       "      <td>[poor, taste, really, disappointed, sorghum, p...</td>\n",
       "      <td>poor tast realli disappoint sorghum purchas us...</td>\n",
       "      <td>poor taste really disappointed sorghum purchas...</td>\n",
       "      <td>poor taste really disappointed sorghum purchas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>better us instant coffee friend gone korea gav...</td>\n",
       "      <td>[better, us, instant, coffee, friend, gone, ko...</td>\n",
       "      <td>better us instant coffe friend gone korea gave...</td>\n",
       "      <td>better u instant coffee friend gone korea gave...</td>\n",
       "      <td>well us instant coffee friend go korea give co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>hard not like no need plastic baggies sloppy t...</td>\n",
       "      <td>[hard, not, like, no, need, plastic, baggies, ...</td>\n",
       "      <td>hard not like no need plastic baggi sloppi tin...</td>\n",
       "      <td>hard not like no need plastic baggies sloppy t...</td>\n",
       "      <td>hard not like no need plastic baggie sloppy ti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score                                             review  \\\n",
       "0      5  very good received product early seller tastey...   \n",
       "1      5  organic kosher tasty assortment premium teas t...   \n",
       "2      5  excellent gluten free spaghetti great taste gr...   \n",
       "3      5  lindt lindt buying multi pack misled picture w...   \n",
       "4      5  yum bars good loved warmed definitely think gr...   \n",
       "5      5  delicious love chips buy 24 pack month bags ri...   \n",
       "6      5  tastes great organic huge fan eating cereal br...   \n",
       "7      2  poor taste really disappointed sorghum purchas...   \n",
       "8      3  better us instant coffee friend gone korea gav...   \n",
       "9      4  hard not like no need plastic baggies sloppy t...   \n",
       "\n",
       "                                               token  \\\n",
       "0  [very, good, received, product, early, seller,...   \n",
       "1  [organic, kosher, tasty, assortment, premium, ...   \n",
       "2  [excellent, gluten, free, spaghetti, great, ta...   \n",
       "3  [lindt, lindt, buying, multi, pack, misled, pi...   \n",
       "4  [yum, bars, good, loved, warmed, definitely, t...   \n",
       "5  [delicious, love, chips, buy, 24, pack, month,...   \n",
       "6  [tastes, great, organic, huge, fan, eating, ce...   \n",
       "7  [poor, taste, really, disappointed, sorghum, p...   \n",
       "8  [better, us, instant, coffee, friend, gone, ko...   \n",
       "9  [hard, not, like, no, need, plastic, baggies, ...   \n",
       "\n",
       "                                                stem  \\\n",
       "0  veri good receiv product earli seller tastey g...   \n",
       "1  organ kosher tasti assort premium tea teasan n...   \n",
       "2  excel gluten free spaghetti great tast great s...   \n",
       "3  lindt lindt buy multi pack misl pictur whole h...   \n",
       "4  yum bar good love warm definit think great sna...   \n",
       "5  delici love chip buy 24 pack month bag right s...   \n",
       "6  tast great organ huge fan eat cereal breakfast...   \n",
       "7  poor tast realli disappoint sorghum purchas us...   \n",
       "8  better us instant coffe friend gone korea gave...   \n",
       "9  hard not like no need plastic baggi sloppi tin...   \n",
       "\n",
       "                                                lemm  \\\n",
       "0  very good received product early seller tastey...   \n",
       "1  organic kosher tasty assortment premium tea te...   \n",
       "2  excellent gluten free spaghetti great taste gr...   \n",
       "3  lindt lindt buying multi pack misled picture w...   \n",
       "4  yum bar good loved warmed definitely think gre...   \n",
       "5  delicious love chip buy 24 pack month bag righ...   \n",
       "6  taste great organic huge fan eating cereal bre...   \n",
       "7  poor taste really disappointed sorghum purchas...   \n",
       "8  better u instant coffee friend gone korea gave...   \n",
       "9  hard not like no need plastic baggies sloppy t...   \n",
       "\n",
       "                                          spacy_lemm  \n",
       "0  very good receive product early seller tastey ...  \n",
       "1  organic kosher tasty assortment premium tea te...  \n",
       "2  excellent gluten free spaghetti great taste gr...  \n",
       "3  lindt lindt buying multi pack mislead picture ...  \n",
       "4  yum bar good love warmed definitely think grea...  \n",
       "5  delicious love chip buy 24 pack month bag righ...  \n",
       "6  taste great organic huge fan eat cereal breakf...  \n",
       "7  poor taste really disappointed sorghum purchas...  \n",
       "8  well us instant coffee friend go korea give co...  \n",
       "9  hard not like no need plastic baggie sloppy ti...  "
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_sub.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'veri good receiv product earli seller tastey great mid day snack share glutten free friend order product soon'"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_sub['stem'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'very good received product early seller tastey great mid day snack shared glutten free friend ordering product soon'"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_sub['lemm'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'very good received product early seller tastey great mid day snack shared glutten free friends ordering product soon'"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_sub['review'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'very good receive product early seller tastey great mid day snack share glutten free friend order product soon'"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_sub['spacy_lemm'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- Vector Space Model and Feature Representation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Count Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain, y_Train, count_vect, feature_names, count_vectDF = VCM(\"count\", final_df_sub, \"stem\", \"Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>24</th>\n",
       "      <th>50</th>\n",
       "      <th>absolut</th>\n",
       "      <th>al</th>\n",
       "      <th>also</th>\n",
       "      <th>amazon</th>\n",
       "      <th>amount</th>\n",
       "      <th>ancient</th>\n",
       "      <th>anoth</th>\n",
       "      <th>are</th>\n",
       "      <th>...</th>\n",
       "      <th>wheat</th>\n",
       "      <th>whole</th>\n",
       "      <th>wife</th>\n",
       "      <th>wonder</th>\n",
       "      <th>would</th>\n",
       "      <th>wrapper</th>\n",
       "      <th>write</th>\n",
       "      <th>year</th>\n",
       "      <th>yum</th>\n",
       "      <th>yunnan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 347 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   24  50  absolut  al  also  amazon  amount  ancient  anoth  are  ...  wheat  \\\n",
       "0   0   0        0   0     0       0       0        0      0    0  ...      0   \n",
       "1   0   0        0   0     1       0       0        1      0    0  ...      0   \n",
       "2   0   0        1   2     0       0       0        0      0    1  ...      1   \n",
       "3   0   0        0   0     0       0       0        0      1    0  ...      0   \n",
       "4   0   0        0   0     0       0       0        0      0    0  ...      0   \n",
       "5   1   0        0   0     0       0       1        0      0    0  ...      0   \n",
       "6   0   0        0   0     0       1       0        0      0    0  ...      0   \n",
       "7   0   1        0   0     0       0       0        0      0    0  ...      0   \n",
       "8   0   0        0   0     1       0       0        0      0    0  ...      0   \n",
       "9   0   0        0   0     0       0       0        0      0    0  ...      0   \n",
       "\n",
       "   whole  wife  wonder  would  wrapper  write  year  yum  yunnan  \n",
       "0      0     0       0      0        0      0     0    0       0  \n",
       "1      0     0       2      0        0      1     0    0       1  \n",
       "2      0     1       0      0        0      0     1    0       0  \n",
       "3      1     0       0      0        0      0     0    0       0  \n",
       "4      0     0       0      0        0      0     0    1       0  \n",
       "5      0     0       0      0        0      0     0    0       0  \n",
       "6      0     0       0      0        0      0     0    0       0  \n",
       "7      0     0       0      0        0      0     1    0       0  \n",
       "8      0     0       0      1        0      0     0    0       0  \n",
       "9      0     0       0      0        1      0     0    0       0  \n",
       "\n",
       "[10 rows x 347 columns]"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectDF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24           0.1\n",
       "past         0.1\n",
       "par          0.1\n",
       "palett       0.1\n",
       "packag       0.1\n",
       "overpow      0.1\n",
       "overcook     0.1\n",
       "peopl        0.1\n",
       "other        0.1\n",
       "opinion      0.1\n",
       "old          0.1\n",
       "nut          0.1\n",
       "notic        0.1\n",
       "note         0.1\n",
       "normal       0.1\n",
       "order        0.1\n",
       "nine         0.1\n",
       "perhap       0.1\n",
       "plastic      0.1\n",
       "rang         0.1\n",
       "quick        0.1\n",
       "purchas      0.1\n",
       "proudli      0.1\n",
       "proof        0.1\n",
       "pronounc     0.1\n",
       "pictur       0.1\n",
       "produc       0.1\n",
       "previous     0.1\n",
       "prevent      0.1\n",
       "pretti       0.1\n",
       "            ... \n",
       "buy          0.3\n",
       "love         0.3\n",
       "breakfast    0.3\n",
       "fill         0.3\n",
       "sweet        0.3\n",
       "box          0.3\n",
       "ingredi      0.3\n",
       "tri          0.4\n",
       "us           0.4\n",
       "coffe        0.4\n",
       "like         0.4\n",
       "no           0.4\n",
       "gluten       0.4\n",
       "snack        0.4\n",
       "perfect      0.4\n",
       "one          0.5\n",
       "pasta        0.5\n",
       "organ        0.5\n",
       "pack         0.5\n",
       "use          0.5\n",
       "herbal       0.5\n",
       "product      0.6\n",
       "teasan       0.6\n",
       "numi         0.6\n",
       "free         0.7\n",
       "good         0.7\n",
       "not          0.8\n",
       "tast         0.9\n",
       "great        0.9\n",
       "tea          1.6\n",
       "Length: 347, dtype: float64"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#visualize important features \n",
    "count_vectDF.mean(axis=0).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 TFIDF Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain2, y_Train2, tfidf_vect, feature_names2, tfidf_vectDF = VCM(\"tfidf\", final_df_sub, \"stem\", \"Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>24</th>\n",
       "      <th>50</th>\n",
       "      <th>absolut</th>\n",
       "      <th>al</th>\n",
       "      <th>also</th>\n",
       "      <th>amazon</th>\n",
       "      <th>amount</th>\n",
       "      <th>ancient</th>\n",
       "      <th>anoth</th>\n",
       "      <th>are</th>\n",
       "      <th>...</th>\n",
       "      <th>wheat</th>\n",
       "      <th>whole</th>\n",
       "      <th>wife</th>\n",
       "      <th>wonder</th>\n",
       "      <th>would</th>\n",
       "      <th>wrapper</th>\n",
       "      <th>write</th>\n",
       "      <th>year</th>\n",
       "      <th>yum</th>\n",
       "      <th>yunnan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035779</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.084177</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.042089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.077206</td>\n",
       "      <td>0.154412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077206</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077206</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077206</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065632</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.193097</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.193097</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.32186</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.207197</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.207197</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.118776</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.21351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181503</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.094250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.11087</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.171688</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 347 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         24       50   absolut        al      also    amazon    amount  \\\n",
       "0  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.00000  0.000000  0.000000  0.035779  0.000000  0.000000   \n",
       "2  0.000000  0.00000  0.077206  0.154412  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "5  0.207197  0.00000  0.000000  0.000000  0.000000  0.000000  0.207197   \n",
       "6  0.000000  0.00000  0.000000  0.000000  0.000000  0.118776  0.000000   \n",
       "7  0.000000  0.21351  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "8  0.000000  0.00000  0.000000  0.000000  0.094250  0.000000  0.000000   \n",
       "9  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "    ancient     anoth       are  ...     wheat     whole      wife    wonder  \\\n",
       "0  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.042089  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.084177   \n",
       "2  0.000000  0.000000  0.077206  ...  0.077206  0.000000  0.077206  0.000000   \n",
       "3  0.000000  0.193097  0.000000  ...  0.000000  0.193097  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "5  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "6  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "7  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "8  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "9  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "     would   wrapper     write      year      yum    yunnan  \n",
       "0  0.00000  0.000000  0.000000  0.000000  0.00000  0.000000  \n",
       "1  0.00000  0.000000  0.042089  0.000000  0.00000  0.042089  \n",
       "2  0.00000  0.000000  0.000000  0.065632  0.00000  0.000000  \n",
       "3  0.00000  0.000000  0.000000  0.000000  0.00000  0.000000  \n",
       "4  0.00000  0.000000  0.000000  0.000000  0.32186  0.000000  \n",
       "5  0.00000  0.000000  0.000000  0.000000  0.00000  0.000000  \n",
       "6  0.00000  0.000000  0.000000  0.000000  0.00000  0.000000  \n",
       "7  0.00000  0.000000  0.000000  0.181503  0.00000  0.000000  \n",
       "8  0.11087  0.000000  0.000000  0.000000  0.00000  0.000000  \n",
       "9  0.00000  0.171688  0.000000  0.000000  0.00000  0.000000  \n",
       "\n",
       "[10 rows x 347 columns]"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectDF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lower         0.004209\n",
       "honeybush     0.004209\n",
       "infus         0.004209\n",
       "jasmin        0.004209\n",
       "king          0.004209\n",
       "known         0.004209\n",
       "lapsang       0.004209\n",
       "highli        0.004209\n",
       "lemongrass    0.004209\n",
       "may           0.004209\n",
       "melang        0.004209\n",
       "mellow        0.004209\n",
       "mist          0.004209\n",
       "monkey        0.004209\n",
       "moroccan      0.004209\n",
       "manufactur    0.004209\n",
       "mountain      0.004209\n",
       "heaven        0.004209\n",
       "harmoni       0.004209\n",
       "dri           0.004209\n",
       "earthi        0.004209\n",
       "encourag      0.004209\n",
       "ever          0.004209\n",
       "experi        0.004209\n",
       "explor        0.004209\n",
       "heal          0.004209\n",
       "fertil        0.004209\n",
       "finest        0.004209\n",
       "fire          0.004209\n",
       "floral        0.004209\n",
       "                ...   \n",
       "yum           0.032186\n",
       "definit       0.032186\n",
       "influenst     0.032186\n",
       "right         0.032209\n",
       "like          0.033391\n",
       "delici        0.034029\n",
       "need          0.034338\n",
       "cereal        0.035633\n",
       "fill          0.035633\n",
       "breakfast     0.035902\n",
       "size          0.036464\n",
       "pasta         0.038603\n",
       "chocol        0.038619\n",
       "bag           0.038805\n",
       "buy           0.042540\n",
       "use           0.044263\n",
       "coffe         0.044348\n",
       "love          0.045090\n",
       "no            0.045641\n",
       "receiv        0.049838\n",
       "not           0.050986\n",
       "pack          0.052483\n",
       "lindt         0.057929\n",
       "free          0.061969\n",
       "snack         0.063819\n",
       "tea           0.067342\n",
       "great         0.077094\n",
       "good          0.077512\n",
       "product       0.080577\n",
       "tast          0.081228\n",
       "Length: 347, dtype: float64"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#visualize important features \n",
    "tfidf_vectDF.mean(axis=0).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'> here we can note that some features although are important, but they are irrelevent in 1 gram and may need bi gram to have good meaning example: taste it can represent either good or bad taste </FONT>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Count Vector WITH BIGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain_ngram_2, y_Train_ngram_2, count_vect_ngram_2, feature_names_ngram_2, count_vectDF_ngram_2 = VCM(\"count\", final_df_sub, \"stem\", \"Score\",2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>24 pack</th>\n",
       "      <th>50 year</th>\n",
       "      <th>absolut perfect</th>\n",
       "      <th>al dent</th>\n",
       "      <th>also call</th>\n",
       "      <th>also thought</th>\n",
       "      <th>amazon carri</th>\n",
       "      <th>amount flavor</th>\n",
       "      <th>ancient one</th>\n",
       "      <th>anoth proof</th>\n",
       "      <th>...</th>\n",
       "      <th>wife italian</th>\n",
       "      <th>wonder assort</th>\n",
       "      <th>wonder infus</th>\n",
       "      <th>would prefer</th>\n",
       "      <th>wrapper honey</th>\n",
       "      <th>write mist</th>\n",
       "      <th>year old</th>\n",
       "      <th>year previous</th>\n",
       "      <th>yum bar</th>\n",
       "      <th>yunnan black</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 514 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   24 pack  50 year  absolut perfect  al dent  also call  also thought  \\\n",
       "0        0        0                0        0          0             0   \n",
       "1        0        0                0        0          1             0   \n",
       "2        0        0                1        2          0             0   \n",
       "3        0        0                0        0          0             0   \n",
       "4        0        0                0        0          0             0   \n",
       "5        1        0                0        0          0             0   \n",
       "6        0        0                0        0          0             0   \n",
       "7        0        1                0        0          0             0   \n",
       "8        0        0                0        0          0             1   \n",
       "9        0        0                0        0          0             0   \n",
       "\n",
       "   amazon carri  amount flavor  ancient one  anoth proof  ...  wife italian  \\\n",
       "0             0              0            0            0  ...             0   \n",
       "1             0              0            1            0  ...             0   \n",
       "2             0              0            0            0  ...             1   \n",
       "3             0              0            0            1  ...             0   \n",
       "4             0              0            0            0  ...             0   \n",
       "5             0              1            0            0  ...             0   \n",
       "6             1              0            0            0  ...             0   \n",
       "7             0              0            0            0  ...             0   \n",
       "8             0              0            0            0  ...             0   \n",
       "9             0              0            0            0  ...             0   \n",
       "\n",
       "   wonder assort  wonder infus  would prefer  wrapper honey  write mist  \\\n",
       "0              0             0             0              0           0   \n",
       "1              1             1             0              0           1   \n",
       "2              0             0             0              0           0   \n",
       "3              0             0             0              0           0   \n",
       "4              0             0             0              0           0   \n",
       "5              0             0             0              0           0   \n",
       "6              0             0             0              0           0   \n",
       "7              0             0             0              0           0   \n",
       "8              0             0             1              0           0   \n",
       "9              0             0             0              1           0   \n",
       "\n",
       "   year old  year previous  yum bar  yunnan black  \n",
       "0         0              0        0             0  \n",
       "1         0              0        0             1  \n",
       "2         1              0        0             0  \n",
       "3         0              0        0             0  \n",
       "4         0              0        1             0  \n",
       "5         0              0        0             0  \n",
       "6         0              0        0             0  \n",
       "7         0              1        0             0  \n",
       "8         0              0        0             0  \n",
       "9         0              0        0             0  \n",
       "\n",
       "[10 rows x 514 columns]"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectDF_ngram_2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24 pack               0.1\n",
       "proudli bring         0.1\n",
       "proof import          0.1\n",
       "pronounc could        0.1\n",
       "product tremend       0.1\n",
       "product though        0.1\n",
       "product soon          0.1\n",
       "product get           0.1\n",
       "product free          0.1\n",
       "product earli         0.1\n",
       "produc gluten         0.1\n",
       "problem chocol        0.1\n",
       "price right           0.1\n",
       "price perfect         0.1\n",
       "previous bought       0.1\n",
       "prevent eat           0.1\n",
       "pretti good           0.1\n",
       "prefer well           0.1\n",
       "poor tast             0.1\n",
       "pleasant certainli    0.1\n",
       "plastic baggi         0.1\n",
       "pictur whole          0.1\n",
       "perhap larger         0.1\n",
       "perfect packag        0.1\n",
       "perfect one           0.1\n",
       "perfect combin        0.1\n",
       "perfect big           0.1\n",
       "peopl sooth           0.1\n",
       "publix discontinu     0.1\n",
       "publix sale           0.1\n",
       "                     ... \n",
       "great great           0.1\n",
       "great amount          0.1\n",
       "yunnan black          0.1\n",
       "good receiv           0.1\n",
       "gone korea            0.1\n",
       "good love             0.1\n",
       "good fast             0.1\n",
       "good found            0.1\n",
       "grain two             0.1\n",
       "instant coffe         0.2\n",
       "high qualiti          0.2\n",
       "al dent               0.2\n",
       "numi tea              0.2\n",
       "us instant            0.2\n",
       "tast good             0.2\n",
       "found brand           0.2\n",
       "premium tea           0.2\n",
       "celiac diseas         0.2\n",
       "great structur        0.2\n",
       "good fill             0.2\n",
       "receiv product        0.2\n",
       "organ kosher          0.2\n",
       "base pasta            0.2\n",
       "no need               0.2\n",
       "great tast            0.2\n",
       "tast great            0.3\n",
       "black tea             0.3\n",
       "green tea             0.3\n",
       "gluten free           0.4\n",
       "herbal teasan         0.4\n",
       "Length: 514, dtype: float64"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectDF_ngram_2.mean(axis=0).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <FONT COLOR=\"RED\"> here we can see the combination of taste with other word how it has a clear meaning </FONT>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:652: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:542: FutureWarning: From version 0.22, errors during fit will result in a cross validation score of NaN by default. Use error_score='raise' if you want an exception raised or error_score=np.nan to adopt the behavior from version 0.22.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-254-c84af2670543>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_Train_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_Train_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Mean cross-validation accuracy: {:.2f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    400\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m                                 error_score=error_score)\n\u001b[0m\u001b[0;32m    403\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    238\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m             error_score=error_score)\n\u001b[1;32m--> 240\u001b[1;33m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    915\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1303\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1305\u001b[1;33m                 sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m   1306\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[1;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[0;32m    879\u001b[0m             raise ValueError(\"This solver needs samples of at least 2 classes\"\n\u001b[0;32m    880\u001b[0m                              \u001b[1;34m\" in the data, but the data contains only one\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 881\u001b[1;33m                              \" class: %r\" % classes_[0])\n\u001b[0m\u001b[0;32m    882\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    883\u001b[0m         \u001b[0mclass_weight_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 5"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "scores = cross_val_score(LogisticRegression(max_iter = 10000), x_Train_1, y_Train_1, cv=5)\n",
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
