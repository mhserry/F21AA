{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string,lxml,bs4,nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from warnings import simplefilter\n",
    "\n",
    "def dataframe_optimzer(dataframe):\n",
    "    dataframe.Summary.fillna('', inplace=True)\n",
    "    dataframe.Text.fillna('', inplace=True)\n",
    "    dataframe.drop(['Id','UserId','ProfileName','HelpfulnessNumerator','HelpfulnessDenominator','Time','ProductId'],axis=1,inplace=True)\n",
    "    #Concatinating TEXT and Summary features as per the DR. example. Which is OK as I didnt find obvoius correlation\n",
    "    dataframe['text']=dataframe['Summary']+' '+dataframe['Text'] \n",
    "    dataframe.drop(['Summary','Text'],axis=1,inplace=True)\n",
    "    #dataframe['length']=dataframe['text'].str.len() \n",
    "    return dataframe\n",
    "\n",
    "def tokenizer(text):\n",
    "    no_html = bs4.BeautifulSoup(text,'lxml').get_text()\n",
    "    no_punctuation = [char for char in no_html if char not in string.punctuation]\n",
    "    no_punctuation = ''.join(no_punctuation)\n",
    "    return no_punctuation\n",
    "\n",
    "def stemmer(text):\n",
    "    review = [nltkstemmer.stem(word) for word in text.split()]\n",
    "    review = ' '.join(review) \n",
    "    return review\n",
    "\n",
    "def lemmatizer(text):\n",
    "    review = [nltklem.lemmatize(word,'v') for word in text.lower().split()]\n",
    "    review = ' '.join(review)\n",
    "    #print(review)\n",
    "    return review\n",
    "\n",
    "def classification_report_final (model): \n",
    "    y_pred_final = model.predict(df_test_data['text'])\n",
    "    score_final =accuracy_score(df_test_labels,y_pred_final)\n",
    "    print('Model score on unseen data',score_final)\n",
    "    print (classification_report(df_test_labels,y_pred_final))\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltklem = WordNetLemmatizer()\n",
    "nltkstemmer = PorterStemmer()\n",
    "simplefilter(action='ignore', category=Warning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Reviews to Dataframe...\n",
    "print('Loading reviews...',end='')\n",
    "df = pd.read_csv('train.csv')\n",
    "print('[ok]')\n",
    "print('Loaded {:,} reviews' .format(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns #Deciding on Columns we might need, dropping the rest for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Data Exploration and Visualization: (10%)\n",
    "Provide an initial step to inspect, visualize and analyse the different attributes in your data set.\n",
    "Document your findings and make conclusions for your next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataframe_optimzer(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist()\n",
    "# We need to comment on the graph below, Basically the data is biased towards score 5 . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Score').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Text Processing and Normalization: (20%)\n",
    "Thoroughly experiment with different text processing and normalization alternatives. Explain the\n",
    "trade-off and benefits of using each and justify their effectiveness for the current data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "NOTE FOR THE TEAM\n",
    "This question (Question 2) is all about text normalization (tokenization) , this includes\n",
    "cleaning the text and make it uniform, this is mainly done via removing punctuations, stopword\n",
    "After our call, I noticed some html tags in the text, we need to remove them as well.\n",
    "The catch is they MUST be removed before the pre-processing function , the reason is once we use the function, we loose punctuations, \n",
    "so <br> looks like br , so we cant really figure out that it was in fact and HTML tag.\n",
    "SO I will work on creating a fn to clean the html tags first, then pass it to the tokenization function \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Update, I found a libraries that can do that, you need to install \n",
    "pip install lxml\n",
    "pip install beautifulsoup4\n",
    "Added them to the tokenizer function\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method 1 - Building our own tokenizer (to remove HTML and punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = df.copy() #Backup to work on raw input data if we will need it later..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATASET REDUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTNAT... THIS IS CREATING A SMALLER SUBSET OF DATA, REMOVE THIS LINE TO WORK ON ALL DATA\n",
    "df = df.head(1000)\n",
    "print ( 'Original DB size is',len(df_original),' Current db size',len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Port Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PS = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltkstemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmer(text):\n",
    "    review = [nltkstemmer.stem(word) for word in text.split()]\n",
    "    review = ' '.join(review) \n",
    "    return review\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PS['text']= df_PS['text'].apply(stemmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decided to go with option v, not n for no obvoius reason except my surface laptop is burning hot :)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_LM = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltklem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_LM['text'] = df_LM['text'].apply(lemmatizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison of the 3 datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=15 #set i to the record you wanna see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original['text'].iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'].iloc[i] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PS['text'].loc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_LM['text'].loc[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write our conclusion here...!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector space Model and feature representation: (20%)\n",
    "Experiment with different representation techniques. Document your findings and make\n",
    "conclusions. Show how choosing n-gram features can influence your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVictorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method1 df | CountVectorizer (defualt 'word' analyzer)\n",
    "\n",
    "# Fitting \n",
    "bow_m1 = CountVectorizer().fit(df['text'])\n",
    "#Transform \n",
    "df_small_bow_m1 = bow_m1.transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method2 df | CountVectorizer(our custom analyzer aka Tokenizer)\n",
    "\n",
    "# Fitting \n",
    "bow_m2 = CountVectorizer(analyzer=tokenizer).fit(df['text'])\n",
    "#Transform \n",
    "df_small_bow_m2 = bow_m2.transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method3 df | CountVectorizer(char analyzer)\n",
    "\n",
    "# Fitting \n",
    "bow_m3 = CountVectorizer(analyzer='char').fit(df['text'])\n",
    "#Transform \n",
    "df_small_bow_m3 = bow_m3.transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method4 df | CountVectorizer(stop words )\n",
    "\n",
    "# Fitting \n",
    "bow_m4 = CountVectorizer(stop_words={'english'},analyzer= 'word',ngram_range=(1,2)).fit(df['text'])\n",
    "#Transform \n",
    "df_small_bow_m4 = bow_m4.transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#findings and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Method 1 CV')\n",
    "print('Bag of words CV1',len(bow_m1.vocabulary_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Method 2 CV')\n",
    "print('Bag of words CV2',len(bow_m2.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Method 3 CV')\n",
    "print('Bag of words CV3',len(bow_m3.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Method 4 CV')\n",
    "print('Bag of words CV4',len(bow_m4.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_TFIDF1 = TfidfVectorizer(stop_words={'english'},ngram_range=(1,2))\n",
    "BOW_TFIDF1.fit(df['text'])\n",
    "BOW_TFIDF1.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_TFIDF2 = TfidfVectorizer(analyzer=tokenizer,ngram_range=(1,2))\n",
    "BOW_TFIDF2.fit(df['text'])\n",
    "BOW_TFIDF1.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_TFIDF3 = TfidfVectorizer(ngram_range=(1,2),min_df=2)\n",
    "BOW_TFIDF3.fit(df['text'])\n",
    "BOW_TFIDF1.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Method 1 TFIDF')\n",
    "print('Bag of words TFIDF1',len(BOW_TFIDF1.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Method 2 TFIDF')\n",
    "print('Bag of words TFIDF2',len(BOW_TFIDF2.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Method 3 TFIDF')\n",
    "print('Bag of words TFIDF3',len(BOW_TFIDF3.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training, selection and hyperparameter tuning and evaluation:(20%)\n",
    "You should at least experiment with 3 models and show how you can optimize model\n",
    "parameters using cross validation. For each model discuss your choices of text processing,\n",
    "representation and features from steps 1-3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading test Data\n",
    "df_test_data = dataframe_optimzer(pd.read_csv(\"test.csv\"))\n",
    "df_test_labels = pd.read_csv(\"labels.csv\",usecols=['Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to cross validate unseen data\n",
    "def classification_report_final (model): \n",
    "    y_pred_final = model.predict(df_test_data['text'])\n",
    "    score_final =accuracy_score(df_test_labels,y_pred_final)\n",
    "    print('Model score on unseen data',score_final)\n",
    "    print (classification_report(df_test_labels,y_pred_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building Pipeline and using Gridsearch to optimize hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Supressing Warnings\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=Warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label = df['Score'].copy()\n",
    "df_data = df['text'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_data = dataframe_optimzer(pd.read_csv(\"test.csv\"))\n",
    "df_test_labels = pd.read_csv(\"labels.csv\",usecols=['Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " X_train, X_test, y_train, y_test = train_test_split(df_data, df_label, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [LogisticRegression,MultinomialNB,SGDClassifier]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline(estimator): #not my code\n",
    "    \n",
    "    pipeline_steps = [\n",
    "        \n",
    "        ('cv',CountVectorizer()),\n",
    "        ('tfidf',TfidfTransformer()),\n",
    "        ('classifier',estimator)\n",
    "    ]\n",
    "    \n",
    "    return Pipeline(pipeline_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating disctoinaries for every estmimator to load the optmization paramters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  parameters for Grid Search (CV and TfIdf) - BASE \n",
    "param_grid= {}\n",
    "\n",
    "#Parameters for CountVectorizer\n",
    "param_grid.update({'cv__ngram_range':[(1,1),(1,2),(1,3)]})\n",
    "param_grid.update({'cv__stop_words':[None,'english']})\n",
    "#param_grid.update({'cv__max_df':[1,2]})\n",
    "param_grid.update({'cv__analyzer':[tokenizer,lemmatizer,'word']})\n",
    "\n",
    "\n",
    "#Parameters of TFIDF\n",
    "param_grid.update({'tfidf__use_idf':[True,False]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  parameters for Grid Search (CV and TfIdf) - BASE \n",
    "param_grid= {}\n",
    "\n",
    "#Parameters for CountVectorizer\n",
    "param_grid.update({})\n",
    "param_grid.update({})\n",
    "#param_grid.update({})\n",
    "param_grid.update({})\n",
    "\n",
    "\n",
    "#Parameters of TFIDF\n",
    "param_grid.update({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters for LR\n",
    "param_grid_LR = {}\n",
    "param_grid_LR = {'classifier__C':[0.0001,0.001]}\n",
    "param_grid_LR.update(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for LR\n",
    "param_grid_LR = {}\n",
    "param_grid_LR = {}\n",
    "param_grid_LR.update(param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters for MN\n",
    "param_grid_MN = {}\n",
    "param_grid_MN = {'classifier__alpha':[0.0001]}\n",
    "param_grid_MN.update(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for MN\n",
    "param_grid_MN = {}\n",
    "param_grid_MN = {}\n",
    "param_grid_MN.update(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters for SGD here\n",
    "param_grid_SG={}\n",
    "param_grid_SG.update(param_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = {\n",
    "    LogisticRegression : param_grid_LR,\n",
    "    MultinomialNB : param_grid_MN,\n",
    "    SGDClassifier : param_grid_SG\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for estimator,search_param in estimators.items():\n",
    "    scores=[]\n",
    "    model = create_pipeline(estimator())\n",
    "    search=GridSearchCV(model,search_param,n_jobs=-1)\n",
    "    search.fit(X_train,y_train)\n",
    "    y_pred = search.predict(X_test)\n",
    "    score=accuracy_score(y_test,y_pred)\n",
    "    scores.append(score)\n",
    "    print(estimator.__name__,'scored',score)\n",
    "    print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "    print(search.best_params_)\n",
    "    print (classification_report(y_test,y_pred))\n",
    "    print('\\n')\n",
    "    print('Cross validation with unseen test data  test data , not used in training')\n",
    "    classification_report_final(search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(max_features=10000, max_df=.15)\n",
    "X = vect.fit_transform(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=5, learning_method=\"batch\",\n",
    "                                max_iter=25, random_state=0)\n",
    "\n",
    "document_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"lda.components_.shape: {}\".format(lda.components_.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "feature_names = np.array(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mglearn as mglearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mglearn.tools.print_topics(topics=range(5), feature_names=feature_names,\n",
    "                           sorting=sorting, topics_per_chunk=5, n_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
