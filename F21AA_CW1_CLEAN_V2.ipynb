{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "import seaborn as sb\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from numba import vectorize\n",
    "import unidecode\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "\n",
    "import string,lxml,bs4,nltk\n",
    "from warnings import simplefilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {\n",
    "\"ain't\": \"are not\",\n",
    "\"aren't\": \"am not\",\n",
    "\"can't\": \"can not\",\n",
    "\"can't've\": \"can not have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"I had\",\n",
    "\"i'd've\": \"I would have\",\n",
    "\"i'll\": \"I will\",\n",
    "\"i'll've\": \"I will have\",\n",
    "\"i'm\": \"I am\",\n",
    "\"i've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so is\",\n",
    "\"that'd\": \"that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they had\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prapare(fileName):\n",
    "    \n",
    "    # READ FILE TO DATAFRAME\n",
    "    result = pd.read_csv(fileName)\n",
    "    \n",
    "    # Remove un necessary columns\n",
    "    columns_needed = ['ProductId','Score','Summary','Text']\n",
    "    to_drop_columns = list(x for x in result.columns.tolist() if x not in columns_needed)\n",
    "    result.drop(to_drop_columns,axis=1,inplace=True)\n",
    "    \n",
    "    # Remove Nulls\n",
    "    columns = ['Summary']\n",
    "    for column in columns:\n",
    "        result[column].fillna('', inplace=True)\n",
    "     \n",
    "    # Merge Summary and Text\n",
    "    result['review'] = result['Summary'] + ' ' + result['Text'] \n",
    "    result.drop(['Summary','Text','ProductId'],axis=1,inplace=True)\n",
    "    \n",
    "    # Remove Outliers\n",
    "    result['length'] = result['review'].apply(len)\n",
    "    result[result['length'] < 3000]\n",
    "    result.drop(['length'],axis=1,inplace=True)\n",
    "    \n",
    "    result.drop_duplicates(keep=\"first\",inplace=True)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prapare_test(fileName):\n",
    "    \n",
    "    # READ FILE TO DATAFRAME\n",
    "    result = pd.read_csv(fileName)\n",
    "    \n",
    "     \n",
    "    # Merge Summary and Text\n",
    "    result['review'] = result['Summary'] + ' ' + result['Text'] \n",
    "    result.drop(['Summary','Text','ProductId'],axis=1,inplace=True)\n",
    "    \n",
    "   \n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(text):\n",
    "    \n",
    "    # Transform to Lower characters\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove html tags\n",
    "    sp = BeautifulSoup(text, \"html.parser\")\n",
    "    text = sp.get_text(separator=\" \")\n",
    "    \n",
    "    # Remove accent\n",
    "    text = unidecode.unidecode(text)\n",
    "    \n",
    "    # Remove pucntuations\n",
    "    for punc in string.punctuation.replace(\"'\",\"\"):\n",
    "        if punc in text:\n",
    "            text = text.replace(punc,\" \")\n",
    "    \n",
    "    # Remove Spaces\n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    # Expand short words\n",
    "    for word in text.split():\n",
    "        if word.lower() in contractions:\n",
    "            text = text.replace(word, contractions[word.lower()])\n",
    "    \n",
    "    # Replace '\n",
    "    text = text.replace(\"'\", \" \")\n",
    "    \n",
    "    # Remove Spaces\n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    # Remove stopwords\n",
    "    sWords = set(stopwords.words('english'))\n",
    "    sWords = list(sWords)\n",
    "    deselect_stop_words = ['no', 'not']\n",
    "\n",
    "    for item in deselect_stop_words:\n",
    "        sWords.remove(item)\n",
    "    for word in sWords:\n",
    "        text = text.replace(\" \" + word.lower() + \" \",\" \")\n",
    "        \n",
    "    # Remove Spaces\n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    text = re.sub(\"\\S*\\d\\S*\", \"\", text).strip()\n",
    "         \n",
    "    # Lemmatize words spacy       \n",
    "    #nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    #doc = nlp(text)\n",
    "    #lemmed = [token.lemma_ for token in doc]\n",
    "    #lemmed = ' '.join(lemmed) \n",
    "    \n",
    "    # Lemmatize words wordnet\n",
    "    #lemmatizer = WordNetLemmatizer()\n",
    "    #lemmed = [lemmatizer.lemmatize(word) for word in text.split()]\n",
    "    #lemmed = ' '.join(lemmed) \n",
    "    \n",
    "    \n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemmed = [stemmer.stem(word) for word in text.split()]\n",
    "    stemmed = ' '.join(stemmed) \n",
    "       \n",
    "            \n",
    "    # Tokenize\n",
    "    #tokenized = word_tokenize(stemmed)\n",
    "    \n",
    "    #return tokenized\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading and preparing test data\n",
    "original_df = load_and_prapare('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying text normalization on training data\n",
    "original_df['review'] = original_df['review'].apply(text_processing) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading test Data\n",
    "df_test_data = pd.read_csv(\"test.csv\",usecols=('Summary','Text'))\n",
    "df_test_labels = pd.read_csv(\"labels.csv\",usecols=['Score'])\n",
    "df_test_data['review'] = df_test_data['Summary'] + ' ' + df_test_data['Text'] \n",
    "df_test_data.drop(['Summary','Text'],axis=1,inplace=True)\n",
    "df_test_data['review'] = df_test_data['review'].fillna('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying text normalization on test data\n",
    "df_test_data['review'] = df_test_data['review'].apply(text_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label = original_df['Score']\n",
    "df_data = original_df['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making sure we have the same row count on both test reviews and test labels after applying normalization\n",
    "len(df_data),len(df_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting training data \n",
    "X_train, X_test, y_train, y_test = train_test_split(df_data, df_label, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [LogisticRegression]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline(estimator):\n",
    "    \n",
    "    pipeline_steps = [\n",
    "        \n",
    "        ('cv',CountVectorizer()),\n",
    "        ('tfidf',TfidfTransformer()),\n",
    "        ('classifier',estimator)\n",
    "    ]\n",
    "    \n",
    "    return Pipeline(pipeline_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  parameters for Grid Search (CV and TfIdf) - BASE \n",
    "param_grid= {}\n",
    "\n",
    "#Parameters for CountVectorizer\n",
    "param_grid.update({'cv__ngram_range':[(1,2)]})\n",
    "#param_grid.update({'cv__stop_words':[None,'english']})\n",
    "#param_grid.update({'cv__max_df':[1,2,3,4]})\n",
    "#param_grid.update({'cv__analyzer':['word']})\n",
    "\n",
    "\n",
    "#Parameters of TFIDF\n",
    "#param_grid.update({'tfidf__use_idf':[True,False]})\n",
    "\n",
    "# Parameters for LR\n",
    "param_grid_LR = {'classifier__class_weight' :[{1:0.64 , 2:0.14 , 3:0.08 , 4:0.05 , 5:0.09}]}\n",
    "#param_grid_LR = {'classifier__C':[1]}\n",
    "param_grid_LR = {}\n",
    "param_grid_LR.update(param_grid)\n",
    "\n",
    "# Parameters for MN\n",
    "param_grid_MN = {}\n",
    "#param_grid_MN = {'classifier__alpha':[0.0001,0.001]}\n",
    "param_grid_MN.update(param_grid)\n",
    "\n",
    "#Parameters for SGD here\n",
    "#param_grid_SG={'classifier__loss':['hinge']}\n",
    "#param_grid_SG.update(param_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = {\n",
    "    LogisticRegression : param_grid_LR\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report_final (model): \n",
    "    y_pred_final = model.predict(df_test_data['review'])\n",
    "    score_final =accuracy_score(df_test_labels,y_pred_final)\n",
    "    print('Model score on unseen data',score_final)\n",
    "    print (classification_report(df_test_labels,y_pred_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building pipeline and applying GridsearchCV \n",
    "for estimator,search_param in estimators.items():\n",
    "    scores=[]\n",
    "    model = create_pipeline(estimator())\n",
    "    search=GridSearchCV(model,search_param,n_jobs=-1)\n",
    "    search.fit(X_train,y_train)\n",
    "    y_pred = search.predict(X_test)\n",
    "    score=accuracy_score(y_test,y_pred)\n",
    "    scores.append(score)\n",
    "    print(estimator.__name__,'scored',score)\n",
    "    print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "    print(search.best_params_)\n",
    "    print (classification_report(y_test,y_pred))\n",
    "    print('\\n')\n",
    "    print('Cross validation with unseen test data')\n",
    "    #classification_report_final(search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report_final(search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogisticRegression scored 0.750908832166837\n",
    "Best parameter (CV score=0.751):\n",
    "{'cv__ngram_range': (1, 2)}\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           1       0.67      0.71      0.69      7113\n",
    "           2       0.44      0.18      0.25      4039\n",
    "           3       0.49      0.33      0.39      5872\n",
    "           4       0.51      0.29      0.37     11024\n",
    "           5       0.81      0.96      0.88     49249\n",
    "\n",
    "    accuracy                           0.75     77297\n",
    "   macro avg       0.59      0.49      0.52     77297\n",
    "weighted avg       0.71      0.75      0.72     77297\n",
    "\n",
    "\n",
    "\n",
    "Cross validation with unseen test data\n",
    "\n",
    "Model score on unseen data 0.7706559522636757\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           1       0.71      0.75      0.73     13075\n",
    "           2       0.58      0.25      0.35      7416\n",
    "           3       0.56      0.38      0.46     10647\n",
    "           4       0.57      0.32      0.41     20346\n",
    "           5       0.82      0.96      0.89     90630\n",
    "\n",
    "    accuracy                           0.77    142114\n",
    "   macro avg       0.65      0.53      0.57    142114\n",
    "weighted avg       0.74      0.77      0.74    142114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
