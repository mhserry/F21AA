{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "import seaborn as sb\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from numba import vectorize\n",
    "import unidecode\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "\n",
    "import string,lxml,bs4,nltk\n",
    "from warnings import simplefilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {\n",
    "\"ain't\": \"are not\",\n",
    "\"aren't\": \"am not\",\n",
    "\"can't\": \"can not\",\n",
    "\"can't've\": \"can not have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"I had\",\n",
    "\"i'd've\": \"I would have\",\n",
    "\"i'll\": \"I will\",\n",
    "\"i'll've\": \"I will have\",\n",
    "\"i'm\": \"I am\",\n",
    "\"i've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so is\",\n",
    "\"that'd\": \"that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they had\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prapare(fileName):\n",
    "    \n",
    "    # READ FILE TO DATAFRAME\n",
    "    result = pd.read_csv(fileName)\n",
    "    \n",
    "    # Remove un necessary columns\n",
    "    columns_needed = ['ProductId','Score','Summary','Text']\n",
    "    to_drop_columns = list(x for x in result.columns.tolist() if x not in columns_needed)\n",
    "    result.drop(to_drop_columns,axis=1,inplace=True)\n",
    "    \n",
    "    # Remove Nulls\n",
    "    columns = ['Summary']\n",
    "    for column in columns:\n",
    "        result[column].fillna('', inplace=True)\n",
    "     \n",
    "    # Merge Summary and Text\n",
    "    result['review'] = result['Summary'] + ' ' + result['Text'] \n",
    "    result.drop(['Summary','Text','ProductId'],axis=1,inplace=True)\n",
    "    \n",
    "    # Remove Outliers\n",
    "    result['length'] = result['review'].apply(len)\n",
    "    result[result['length'] < 3000]\n",
    "    result.drop(['length'],axis=1,inplace=True)\n",
    "    \n",
    "    result.drop_duplicates(keep=\"first\",inplace=True)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(text):\n",
    "    \n",
    "    # Transform to Lower characters\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove html tags\n",
    "    sp = BeautifulSoup(text, \"html.parser\")\n",
    "    text = sp.get_text(separator=\" \")\n",
    "    \n",
    "    # Remove accent\n",
    "    text = unidecode.unidecode(text)\n",
    "    \n",
    "    # Remove pucntuations\n",
    "    for punc in string.punctuation.replace(\"'\",\"\"):\n",
    "        if punc in text:\n",
    "            text = text.replace(punc,\" \")\n",
    "    \n",
    "    # Remove Spaces\n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    # Expand short words\n",
    "    for word in text.split():\n",
    "        if word.lower() in contractions:\n",
    "            text = text.replace(word, contractions[word.lower()])\n",
    "    \n",
    "    # Replace '\n",
    "    text = text.replace(\"'\", \" \")\n",
    "    \n",
    "    # Remove Spaces\n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    # Remove stopwords\n",
    "    sWords = set(stopwords.words('english'))\n",
    "    sWords = list(sWords)\n",
    "    deselect_stop_words = ['no', 'not']\n",
    "\n",
    "    for item in deselect_stop_words:\n",
    "        sWords.remove(item)\n",
    "    for word in sWords:\n",
    "        text = text.replace(\" \" + word.lower() + \" \",\" \")\n",
    "        \n",
    "    # Remove Spaces\n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    text = re.sub(\"\\S*\\d\\S*\", \"\", text).strip()\n",
    "         \n",
    "    # Lemmatize words spacy       \n",
    "    #nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    #doc = nlp(text)\n",
    "    #lemmed = [token.lemma_ for token in doc]\n",
    "    #lemmed = ' '.join(lemmed) \n",
    "    \n",
    "    # Lemmatize words wordnet\n",
    "    #lemmatizer = WordNetLemmatizer()\n",
    "    #lemmed = [lemmatizer.lemmatize(word) for word in text.split()]\n",
    "    #lemmed = ' '.join(lemmed) \n",
    "    \n",
    "    \n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemmed = [stemmer.stem(word) for word in text.split()]\n",
    "    stemmed = ' '.join(stemmed) \n",
    "       \n",
    "            \n",
    "    # Tokenize\n",
    "    tokenized = word_tokenize(stemmed)\n",
    "    \n",
    "    return tokenized\n",
    "    #return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = load_and_prapare('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df_sub = original_df.sample(frac=1).head(10000).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipline = Pipeline([\n",
    "    ('bow',CountVectorizer(analyzer=text_processing,max_features=2000,ngram_range=(1,2))),\n",
    "    ('tfidf',TfidfTransformer()),\n",
    "    #('classifier',MultinomialNB())\n",
    "    ('classifier',LogisticRegression())\n",
    "    #('classifier',SGDClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(original_df_sub['review'],original_df_sub['Score'],test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('bow', CountVectorizer(analyzer=<function text_processing at 0x00000258BBF6FD90>,\n",
       "        binary=False, decode_error='strict', dtype=<class 'numpy.int64'>,\n",
       "        encoding='utf-8', input='content', lowercase=True, max_df=1.0,\n",
       "        max_features=2000, min_df=1, ngram_range=(1, 2), preproce...penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipline.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipline.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6896666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.61      0.43      0.50       287\n",
      "           2       0.56      0.03      0.06       169\n",
      "           3       0.37      0.08      0.14       201\n",
      "           4       0.43      0.11      0.18       441\n",
      "           5       0.71      0.99      0.83      1902\n",
      "\n",
      "   micro avg       0.69      0.69      0.69      3000\n",
      "   macro avg       0.53      0.33      0.34      3000\n",
      "weighted avg       0.63      0.69      0.61      3000\n",
      "\n",
      "[[ 123    0   10    5  149]\n",
      " [  37    5   12   16   99]\n",
      " [  21    3   17   26  134]\n",
      " [  15    0    7   50  369]\n",
      " [   7    1    0   20 1874]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, classification_report, confusion_matrix\n",
    "print (\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "#print (\"Precision:\", precision_score(y_test, predictions))\n",
    "print (classification_report(y_test, predictions))\n",
    "print (confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df_sub_2 = original_df.sample(frac=1).head(1000).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(original_df_sub_2['review'],original_df_sub_2['Score'],test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [LogisticRegression,MultinomialNB,SGDClassifier]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline(estimator):\n",
    "    \n",
    "    pipeline_steps = [\n",
    "        \n",
    "        ('cv',CountVectorizer()),\n",
    "        ('tfidf',TfidfTransformer()),\n",
    "        ('classifier',estimator)\n",
    "    ]\n",
    "    \n",
    "    return Pipeline(pipeline_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  parameters for Grid Search (CV and TfIdf) - BASE \n",
    "param_grid= {}\n",
    "\n",
    "#Parameters for CountVectorizer\n",
    "param_grid.update({'cv__ngram_range':[(1,1),(2,2)]})\n",
    "#param_grid.update({'cv__stop_words':[None,'english']})\n",
    "#param_grid.update({'cv__max_df':[1,2,3,4]})\n",
    "param_grid.update({'cv__analyzer':[text_processing]})\n",
    "param_grid.update({'cv__max_features':[2000]})\n",
    "\n",
    "\n",
    "#Parameters of TFIDF\n",
    "param_grid.update({'tfidf__use_idf':[True,False]})\n",
    "\n",
    "# Parameters for LR\n",
    "param_grid_LR = {}\n",
    "param_grid_LR = {'classifier__C':[0.0001]}\n",
    "param_grid_LR.update(param_grid)\n",
    "\n",
    "# Parameters for MN\n",
    "param_grid_MN = {}\n",
    "param_grid_MN = {'classifier__alpha':[0.0001,0.001]}\n",
    "param_grid_MN.update(param_grid)\n",
    "\n",
    "#Parameters for SGD here\n",
    "param_grid_SG={}\n",
    "param_grid_SG.update(param_grid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = {\n",
    "    LogisticRegression : param_grid_LR,\n",
    "    MultinomialNB : param_grid_MN,\n",
    "    SGDClassifier : param_grid_SG\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report_final (model): \n",
    "    y_pred_final = model.predict(df_test_data['review'])\n",
    "    score_final =accuracy_score(df_test_labels,y_pred_final)\n",
    "    print('Model score on unseen data',score_final)\n",
    "    print (classification_report(df_test_labels,y_pred_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_data = load_and_prapare(\"test.csv\").head(1000)\n",
    "df_test_labels = pd.read_csv(\"labels.csv\",usecols=['Score']).head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Love this flavor Fog chaser was the best both ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tasty Tea We found this at PF Changs, and it t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yum Not quite a chocolate bar substitute but d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gluten Free Biscotti This is not as tasty as P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Happy Puppies &lt;a href=\"http://www.amazon.com/g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NASTY TASTING!!!!!!!!!!!!!!!!!!!!!!!!!!! THIS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>This THE BEST formula to use ..by FAR - This i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>My Schnauzer Loves This Food! I have a miniatu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Less Tasty Than Expected. Like other reviewers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Best gummies EVER! I've looked high and low fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review\n",
       "0  Love this flavor Fog chaser was the best both ...\n",
       "1  Tasty Tea We found this at PF Changs, and it t...\n",
       "2  Yum Not quite a chocolate bar substitute but d...\n",
       "3  Gluten Free Biscotti This is not as tasty as P...\n",
       "4  Happy Puppies <a href=\"http://www.amazon.com/g...\n",
       "5  NASTY TASTING!!!!!!!!!!!!!!!!!!!!!!!!!!! THIS ...\n",
       "6  This THE BEST formula to use ..by FAR - This i...\n",
       "7  My Schnauzer Loves This Food! I have a miniatu...\n",
       "8  Less Tasty Than Expected. Like other reviewers...\n",
       "9  Best gummies EVER! I've looked high and low fo..."
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression scored 0.6333333333333333\n",
      "Best parameter (CV score=0.649):\n",
      "{'classifier__C': 0.0001, 'cv__analyzer': <function text_processing at 0x00000258BBF6FD90>, 'cv__max_features': 2000, 'cv__ngram_range': (1, 1), 'tfidf__use_idf': True}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00        26\n",
      "           2       0.00      0.00      0.00        14\n",
      "           3       0.00      0.00      0.00        20\n",
      "           4       0.00      0.00      0.00        50\n",
      "           5       0.63      1.00      0.78       190\n",
      "\n",
      "   micro avg       0.63      0.63      0.63       300\n",
      "   macro avg       0.13      0.20      0.16       300\n",
      "weighted avg       0.40      0.63      0.49       300\n",
      "\n",
      "\n",
      "\n",
      "Cross validation with unseen test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model score on unseen data 0.682\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00        80\n",
      "           2       0.00      0.00      0.00        46\n",
      "           3       0.00      0.00      0.00        71\n",
      "           4       0.00      0.00      0.00       121\n",
      "           5       0.68      1.00      0.81       682\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      1000\n",
      "   macro avg       0.14      0.20      0.16      1000\n",
      "weighted avg       0.47      0.68      0.55      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB scored 0.64\n",
      "Best parameter (CV score=0.654):\n",
      "{'classifier__alpha': 0.001, 'cv__analyzer': <function text_processing at 0x00000258BBF6FD90>, 'cv__max_features': 2000, 'cv__ngram_range': (1, 1), 'tfidf__use_idf': False}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.08      0.14        26\n",
      "           2       0.00      0.00      0.00        14\n",
      "           3       0.00      0.00      0.00        20\n",
      "           4       0.40      0.04      0.07        50\n",
      "           5       0.65      0.99      0.78       190\n",
      "\n",
      "   micro avg       0.64      0.64      0.64       300\n",
      "   macro avg       0.41      0.22      0.20       300\n",
      "weighted avg       0.56      0.64      0.52       300\n",
      "\n",
      "\n",
      "\n",
      "Cross validation with unseen test data\n",
      "Model score on unseen data 0.671\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.22      0.03      0.04        80\n",
      "           2       0.00      0.00      0.00        46\n",
      "           3       0.00      0.00      0.00        71\n",
      "           4       0.11      0.01      0.02       121\n",
      "           5       0.68      0.98      0.80       682\n",
      "\n",
      "   micro avg       0.67      0.67      0.67      1000\n",
      "   macro avg       0.20      0.20      0.17      1000\n",
      "weighted avg       0.50      0.67      0.55      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier scored 0.6466666666666666\n",
      "Best parameter (CV score=0.643):\n",
      "{'cv__analyzer': <function text_processing at 0x00000258BBF6FD90>, 'cv__max_features': 2000, 'cv__ngram_range': (2, 2), 'tfidf__use_idf': True}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.56      0.38      0.45        26\n",
      "           2       0.00      0.00      0.00        14\n",
      "           3       0.00      0.00      0.00        20\n",
      "           4       0.41      0.34      0.37        50\n",
      "           5       0.74      0.88      0.80       190\n",
      "\n",
      "   micro avg       0.65      0.65      0.65       300\n",
      "   macro avg       0.34      0.32      0.33       300\n",
      "weighted avg       0.59      0.65      0.61       300\n",
      "\n",
      "\n",
      "\n",
      "Cross validation with unseen test data\n",
      "Model score on unseen data 0.542\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.17      0.10      0.13        80\n",
      "           2       0.02      0.02      0.02        46\n",
      "           3       0.14      0.07      0.09        71\n",
      "           4       0.13      0.13      0.13       121\n",
      "           5       0.69      0.75      0.72       682\n",
      "\n",
      "   micro avg       0.54      0.54      0.54      1000\n",
      "   macro avg       0.23      0.22      0.22      1000\n",
      "weighted avg       0.51      0.54      0.52      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for estimator,search_param in estimators.items():\n",
    "    scores=[]\n",
    "    model = create_pipeline(estimator())\n",
    "    search=GridSearchCV(model,search_param,n_jobs=-1)\n",
    "    search.fit(x_train,y_train)\n",
    "    y_pred = search.predict(x_test)\n",
    "    score=accuracy_score(y_test,y_pred)\n",
    "    scores.append(score)\n",
    "    print(estimator.__name__,'scored',score)\n",
    "    print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "    print(search.best_params_)\n",
    "    print (classification_report(y_test,y_pred))\n",
    "    print('\\n')\n",
    "    print('Cross validation with unseen test data')\n",
    "    classification_report_final(search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
